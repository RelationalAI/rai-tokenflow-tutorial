{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a367d3",
   "metadata": {},
   "source": [
    "# Link predicyion example using the GNN learning engine under the RelationalAI Snowflake Native App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c82308",
   "metadata": {},
   "source": [
    "## ‚úÖ Prerequisites\n",
    "\n",
    "Before running this notebook, make sure you've completed the following setup steps:\n",
    "\n",
    "1. **Install the RelationalAI Experimental Application**\n",
    "   Ensure that the RelationalAI experimental app is installed in your Snowflake account.\n",
    "\n",
    "2. **Preprocess the Tokenflow Dataset**\n",
    "   Run the `preprocess_tokenflow_data.ipynb` notebook to generate the processed dataset required for this tutorial.\n",
    "\n",
    "3. **Upload the Preprocessed Data to Snowflake**\n",
    "   Use the preprocessing notebook to upload the Tokenflow data to the appropriate tables in your Snowflake environment.\n",
    "\n",
    "4. **Install the RelationalAI GNN Python SDK**\n",
    "   Follow the installation steps provided in the [README.md](GNN_SDK_DOCUMENTATION.md) to set up the RelationalAI GNN SDK in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b9883",
   "metadata": {},
   "source": [
    "### üîê Connecting to the RelationalAI Native App via Snowflake\n",
    "\n",
    "To connect to the RelationalAI native app, we'll first set up a `.env` file that defines the necessary environment variables for establishing a Snowflake connection.\n",
    "\n",
    "We assume you have created a `.env` file in the same directory as this notebook. (A sample `.env` file is included for reference.)\n",
    "\n",
    "Your `.env` file should contain the following fields:\n",
    "\n",
    "```\n",
    "ACCOUNT_NAME=snowflake_account_name\n",
    "USER_NAME=snowflake_user_name\n",
    "PASSWORD=snowflake_user_password\n",
    "WAREHOUSE=snowflake_warehouse\n",
    "APP_NAME=RAI_EXPT_APP\n",
    "AUTH_METHOD=password\n",
    "```\n",
    "\n",
    "#### üîé Need help finding your Snowflake account name?\n",
    "\n",
    "You can retrieve your account identifier by running the following SQL command in Snowflake:\n",
    "\n",
    "```sql\n",
    "SELECT CURRENT_ORGANIZATION_NAME() || '-' || CURRENT_ACCOUNT_NAME();\n",
    "```\n",
    "\n",
    "#### üîê About Authentication\n",
    "\n",
    "In this tutorial, we‚Äôll use **password authentication**. However, other authentication methods‚Äîsuch as **Key Pair Authentication** or **OAuth Token Authentication**‚Äîare also supported. For more details, refer to the [RelationalAI GNN documentation](GNN_SDK_DOCUMENTATION.md).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the snowflake configuration to a python dict\n",
    "snowflake_config = {\n",
    "    \"account\": os.getenv(\"ACCOUNT_NAME\"),\n",
    "    \"user\": os.getenv(\"USER_NAME\"),\n",
    "    \"password\": os.getenv(\"PASSWORD\"),\n",
    "    \"warehouse\": os.getenv(\"WAREHOUSE\"),\n",
    "    \"app_name\": os.getenv(\"APP_NAME\"),\n",
    "    \"auth_method\": os.getenv(\"AUTH_METHOD\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089bcd4",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öôÔ∏è Managing Your GNN Engines\n",
    "\n",
    "The `Provider` class in the RelationalAI GNN SDK allows you to manage your GNN engines seamlessly. Below, we walk through common operations you can perform with the `Provider`:\n",
    "\n",
    "* ‚úÖ Create a new GNN engine\n",
    "* üìã List all available engines\n",
    "* üîç Check the status of an engine\n",
    "* üîÑ Resume a paused engine\n",
    "* ‚ùå Delete an engine\n",
    "\n",
    "Each of these operations can be done with simple method calls, as shown in the following examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rai_gnns_experimental import Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e38af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the provider using the snowflake configuration\n",
    "# (note: you might be prompted from your MFA app at this point)\n",
    "provider = Provider(**snowflake_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f4da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the available engines.\n",
    "provider.list_gnns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14702efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create a new GNN engine, currently two\n",
    "# engine types are supported \"GPU_NV_S\" and \"HIGHMEM_X64_S\"\n",
    "# here we create an engine by specifying the engine name (a\n",
    "# custom name) and also the type of the engine, specified by the\n",
    "# \"size\" parameter\n",
    "provider.create_gnn(name=\"my_engine\", size=\"GPU_NV_S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7636da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can directly see the status of the engine\n",
    "# the status of the engine 'READY' marks the fact that the\n",
    "# engine is ready to be used. A `PENDING` status marks the\n",
    "# fact that the engine has  been automaticaly suspended.\n",
    "# Notice also that under the settings \n",
    "# the provider exposes a URL for the MLFLOW endpoint\n",
    "# that we can use to track our experiments\n",
    "provider.get_gnn(\"my_engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6182e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: We can always resume a GNN engine that has been suspended:\n",
    "# Note: Engine provisioning can take some minutes. Please\n",
    "# check the engine status using provider.get_gnn(name=\"my_engine\")\n",
    "\n",
    "# provider.resume_gnn(name=\"my_engine\")\n",
    "\n",
    "# And if we need we can also delete a GNN engine\n",
    "\n",
    "# provider.delete_gnn(name=\"ilias_test_engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6a61a",
   "metadata": {},
   "source": [
    "## üîå Connector Setup\n",
    "\n",
    "The `Connector` class, like the `Provider` class, is used to communicate with Snowflake. However, while the `Provider` is responsible for managing GNN engines, the `Connector` is specifically used to interface with the **GNN learning engine** itself.\n",
    "\n",
    "You‚Äôll use the `Connector` instance as an input to all SDK components that need to send requests to the GNN engine‚Äîsuch as loading data, running training jobs, or performing inference.\n",
    "\n",
    "In short:\n",
    "\n",
    "* `Provider` ‚Üí Manages GNN engine instances (create, list, delete, etc.)\n",
    "* `Connector` ‚Üí Sends requests to a specific GNN engine for processing tasks\n",
    "\n",
    "Let‚Äôs now walk through how to create and use a `Connector`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rai_gnns_experimental import SnowflakeConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we initialize the connector and passing all our credentials.\n",
    "connector = SnowflakeConnector(\n",
    "    **snowflake_config,\n",
    "    engine_name=\"my_engine\",\n",
    ")\n",
    "# the connector also provides access to MLFLOW that you can\n",
    "# use to monitor your experiments and register trained GNN models\n",
    "connector.mlflow_session_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18457ac",
   "metadata": {},
   "source": [
    "## üìä Preparing the data: Creating the GNN tables\n",
    "\n",
    "In this section, we will define the GNN tables and the associated learning task. These components will then be used to construct a GNN dataset suitable for training.\n",
    "\n",
    "For this tutorial, we‚Äôll use the Tokenflow database as our working example. This database includes three tables: BUYERS, SENDERS, and TRANSACTIONS. The TRANSACTIONS table links BUYERS to SENDERS. Our objective is to predict which BUYERS a given SENDER is most likely to transact with in the future‚Äîframing this as a link prediction task.\n",
    "\n",
    "Before loading the data, ensure that your application has the necessary permissions to access the relevant tables. In Snowflake, this can be achieved by running the following commands:\n",
    "\n",
    "```sql\n",
    "-- here we grant access to all schemas and tables, you might want to\n",
    "-- select specific tables and schemas to grant access to\n",
    "GRANT USAGE ON DATABASE GNN_TOKENFLOW TO APPLICATION RELATIONALAI;\n",
    "GRANT USAGE ON ALL SCHEMAS IN DATABASE GNN_TOKENFLOW TO APPLICATION RELATIONALAI;\n",
    "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN DATABASE GNN_TOKENFLOW TO APPLICATION RELATIONALAI;\n",
    "-- grant write access to write results, we encourage the user to select specific schemas\n",
    "-- to give write access to\n",
    "GRANT CREATE TABLE ON ALL SCHEMAS IN DATABASE GNN_TOKENFLOW TO APPLICATION RELATIONALAI;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rai_gnns_experimental import GNNTable\n",
    "from rai_gnns_experimental import ForeignKey\n",
    "from rai_gnns_experimental import ColumnDType\n",
    "\n",
    "# create a table for the buyers and set the \n",
    "# BUY_TOKEN_ADDRESS as a primary key (primary and \n",
    "# foreign keys are used to construct the edges of the graph)\n",
    "buyers_table = GNNTable(\n",
    "    connector=connector,\n",
    "    source=\"GNN_TOKENFLOW.DATA.BUYERS\",\n",
    "    name=\"buyers\",\n",
    "    primary_key=\"BUY_TOKEN_ADDRESS\",\n",
    ")\n",
    "buyers_table.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb112ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a similar manner we can create the SENDERS table\n",
    "senders_table = GNNTable(\n",
    "    connector=connector,\n",
    "    source=\"GNN_TOKENFLOW.DATA.SENDERS\",\n",
    "    name=\"senders\",\n",
    "    primary_key=\"TX_SENDER_ADDRESS\",\n",
    ")\n",
    "senders_table.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352126e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally we will link the two tables using the foreign\n",
    "# keys from the TRANSACTIONS table. Note: the transactions\n",
    "# table  has also one special \"time column\" that will be used\n",
    "# to prevent data leakage (see the documentation for more details)\n",
    "transactions_table = GNNTable(\n",
    "    connector=connector,\n",
    "    source=\"GNN_TOKENFLOW.DATA.TRANSACTIONS\",\n",
    "    name=\"transactions\",\n",
    "    foreign_keys=[\n",
    "        ForeignKey(\n",
    "            column_name=\"TX_SENDER_ADDRESS\", link_to=\"senders.TX_SENDER_ADDRESS\"\n",
    "        ),\n",
    "        ForeignKey(column_name=\"BUY_TOKEN_ADDRESS\", link_to=\"buyers.BUY_TOKEN_ADDRESS\"),\n",
    "    ],\n",
    "    time_column=\"BLOCK_TIMESTAMP\",\n",
    ")\n",
    "# modify column dtypes if needed\n",
    "transactions_table.update_column_dtype(\n",
    "    col_name=\"BUY_AMOUNT\", dtype=ColumnDType.integer_t\n",
    ")\n",
    "transactions_table.update_column_dtype(\n",
    "    col_name=\"SELL_AMOUNT\", dtype=ColumnDType.integer_t\n",
    ")\n",
    "transactions_table.show_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73738b6",
   "metadata": {},
   "source": [
    "\n",
    "## üîß Preparing the Data: Creating the Task\n",
    "\n",
    "To define the task, we begin by specifying the locations of the training, validation, and test datasets. We also identify the source and destination entity tables, along with the corresponding columns that uniquely identify each entity.\n",
    "\n",
    "Since this is a **link prediction** task, our objective is to predict future connections between a source entity and a destination entity.\n",
    "\n",
    "Additionally, we define a **timestamp column** to avoid information leakage by ensuring that future data doesn't influence past predictions (see [documentation](GNN_SDK_DOCUMENTATION.md) for details). Lastly, we specify the **evaluation metric**‚Äîin this case, **Mean Average Precision (MAP)**‚Äîto assess the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rai_gnns_experimental import LinkTask\n",
    "from rai_gnns_experimental import TaskType\n",
    "from rai_gnns_experimental import EvaluationMetric\n",
    "\n",
    "link_pred_task = LinkTask(\n",
    "    connector=connector,\n",
    "    name=\"sender_buyer\",\n",
    "    task_data_source={\n",
    "        \"train\": \"GNN_TOKENFLOW.TASK.TRAIN\", \n",
    "        \"test\": \"GNN_TOKENFLOW.TASK.TEST\", \n",
    "        \"validation\": \"GNN_TOKENFLOW.TASK.VALIDATION\"\n",
    "    },\n",
    "    # name of source entity column that we want to do predictions for\n",
    "    source_entity_column=\"TX_SENDER_ADDRESS\",\n",
    "    # name of GNN table that column is at\n",
    "    source_entity_table=\"senders\",\n",
    "    # name of target entity column that we want to predict\n",
    "    target_entity_column=\"BUY_TOKEN_ADDRESS\",\n",
    "    # name of GNN table that column is at\n",
    "    target_entity_table=\"buyers\",\n",
    "    time_column=\"BLOCK_TIMESTAMP\",\n",
    "    task_type=TaskType.LINK_PREDICTION,\n",
    "    evaluation_metric=EvaluationMetric(name=\"link_prediction_map\", eval_at_k=12),\n",
    ")\n",
    "link_pred_task.show_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c399774",
   "metadata": {},
   "source": [
    "## üß© Preparing the Data: Creating the Dataset\n",
    "\n",
    "Finally, we combine all the components by constructing a dataset object that encapsulates both the GNN tables and the task definition. This dataset will serve as the input to the model training pipeline, ensuring that the task and its associated data are tightly integrated and ready for downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84803044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rai_gnns_experimental import Dataset\n",
    "\n",
    "dataset = Dataset(\n",
    "    connector=connector,\n",
    "    dataset_name=\"tokenflow\",\n",
    "    tables=[buyers_table, transactions_table, senders_table],\n",
    "    task_description=link_pred_task,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also visualize the dataset \n",
    "# (Note: this requires Python 3.11)\n",
    "from IPython.display import Image, display\n",
    "\n",
    "graph = dataset.visualize_dataset(show_dtypes=True)\n",
    "graph.set_graph_defaults(size=\"50,50!\")  # Increase graph size\n",
    "plt = Image(graph.create_png(), width=600, height=600)\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33825942",
   "metadata": {},
   "source": [
    "## üöÄ GNN Model Training\n",
    "\n",
    "Now that our dataset is ready, we can train our first GNN model. We‚Äôll begin by defining a **configuration** that specifies the training parameters, such as model architecture, optimizer settings, and training duration.\n",
    "\n",
    "Next, we‚Äôll instantiate a **trainer** using this configuration. The trainer will consume the dataset we previously created and manage the entire training process. By calling the `fit()` method on the trainer, we initiate a training job‚Äîwhose progress and status can be monitored throughout execution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60916d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rai_gnns_experimental import TrainerConfig\n",
    "from rai_gnns_experimental import Trainer\n",
    "\n",
    "# the first step will be to define a configuration for our Trainer.\n",
    "# the configuration includes many parameters that are explained in\n",
    "# detail in the documentation. It does not only provide parameters\n",
    "# for the graph neural network but also parameters for other components\n",
    "# of the model (such as feature extractors, prediction head parameters,\n",
    "# training parameters etc.)\n",
    "model_config = TrainerConfig(\n",
    "    connector=connector,\n",
    "    device=\"cuda\",  # either 'cuda' or 'cpu'\n",
    "    n_epochs=3,\n",
    "    max_iters=200,\n",
    "    text_embedder=\"model2vec-potion-base-4M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we initialize now our trainer object with the trainer configuration\n",
    "# the trainer object can be used to train a model, to perform inference\n",
    "# or to perform training & inference.\n",
    "trainer = Trainer(connector=connector, config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dfa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our first example we will use the trainer to perform training only.\n",
    "# every time the trainer is \"executed\" (calling fit(), predict() or fit_predict())\n",
    "# it returns a job object that can be used to monitor the current running job.\n",
    "# See the documentation for the meaning of the job statuses\n",
    "train_job = trainer.fit(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can monitor the job status\n",
    "# observe that once the job is running we also get back an experiment name\n",
    "# we will see later how we can use that to perform inference\n",
    "train_job.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362daff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also stream the logs of the training job in real time\n",
    "# Hint: You can stop the cell execution to stop monitoring of logs\n",
    "# Hint: At this point you can also open the MLFLow URL to monitor your experiments\n",
    "train_job.stream_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: one can cancel a running job as well\n",
    "# train_job.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rai_gnns_experimental import JobManager\n",
    "# it might be the case that we have lost track of\n",
    "# the jobs that we are running. To this end we also\n",
    "# provide to the user a JobManager object that can\n",
    "# give us the status of all current jobs. Let's see\n",
    "# an example:\n",
    "job_manager = JobManager(connector=connector)\n",
    "job_manager.show_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df60190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also get back job details using it's job id\n",
    "foo_job = job_manager.fetch_job(\"01bd3a00-0000-21ef-001d-cf8b0009ea52\")\n",
    "# hint: the job manager can be used to cancel any job as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420679f1",
   "metadata": {},
   "source": [
    "## üîç Inference Using a Trained Model\n",
    "\n",
    "Finally, we‚Äôll demonstrate how to perform inference using the model we‚Äôve just trained. In this example, we'll directly use the recently trained model to generate predictions.\n",
    "\n",
    "For more advanced use cases‚Äîsuch as registering a model for reuse or automatically selecting the best-performing model‚Äîplease refer to the [documentation](GNN_SDK_DOCUMENTATION.md) for further guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rai_gnns_experimental import OutputConfig\n",
    "\n",
    "output_config = OutputConfig.snowflake(database_name=\"GNN_TOKENFLOW\", schema_name=\"PUBLIC\")\n",
    "\n",
    "inference_job = trainer.predict(\n",
    "    output_alias=\"TEST_PREDS\",\n",
    "    output_config=output_config,\n",
    "    test_batch_size=128,\n",
    "    dataset=dataset,\n",
    "    model_run_id=train_job.model_run_id,\n",
    "    extract_embeddings=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_job.stream_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32114d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rai_gnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
