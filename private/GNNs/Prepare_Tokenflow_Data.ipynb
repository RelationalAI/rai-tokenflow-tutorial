{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a38f33e",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Tokenflow Dataset Preparation for Link Prediction\n",
    "\n",
    "This notebook provides the necessary preprocessing steps to prepare the **Tokenflow** dataset for use with the **RelationalAI Graph Neural Network (GNN) learning engine**.\n",
    "\n",
    "We specifically format the dataset for a **link prediction task**, where the goal is to predict relationships (edges) between entities in a graph.\n",
    "\n",
    "Once the preprocessing is complete, the processed data will be uploaded to the appropriate **Snowflake tables**, ready to be consumed by the GNN engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb52d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd3e54",
   "metadata": {},
   "source": [
    "## ðŸ”— Link Prediction Use Case: Predicting Transactions\n",
    "\n",
    "In our link prediction task, the goal is to predict **transaction links** between **senders** and **buyers**.\n",
    "\n",
    "To prepare the data for this task, we will follow these steps:\n",
    "\n",
    "1. **Create Entity Tables**\n",
    "   We will create two tablesâ€”one for **buyers** and one for **senders**â€”containing the unique IDs of each entity type.\n",
    "\n",
    "2. **Create the Relationship (Edge) Table**\n",
    "   We will use `token-trades.csv` as our **transaction table**, representing the edges (links) between senders and buyers.\n",
    "\n",
    "3. **Generate Train, Validation, and Test Splits**\n",
    "   Using the transaction data, we will create separate datasets for training, validation, and testing.\n",
    "\n",
    "This setup ensures the data is properly formatted and ready for ingestion by the RelationalAI GNN engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trades_df = pd.read_csv('../../data/token-trades.csv')\n",
    "trades_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two dataframes with unique buyer and sender IDs\n",
    "buyers_df = trades_df[['BUY_TOKEN_ADDRESS']].drop_duplicates()\n",
    "senders_df = trades_df[['TX_SENDER_ADDRESS']].drop_duplicates()\n",
    "# let's make a transaction table from the trades table by keeping some\n",
    "# of the features\n",
    "transactions_df = trades_df[['TX_SENDER_ADDRESS','BUY_TOKEN_ADDRESS','BLOCK_TIMESTAMP',\n",
    "                             'BUY_AMOUNT','BUY_TOKEN_SYMBOL','SELL_TOKEN_SYMBOL',\n",
    "                             'SELL_AMOUNT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df['BLOCK_TIMESTAMP'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36baf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, test and validation data\n",
    "train_start_date = \"2024-10-21\"\n",
    "train_end_date = \"2025-01-14\"\n",
    "val_end_date = \"2025-01-31\"\n",
    "\n",
    "# Ensure BLOCK_TIMESTAMP is in datetime format\n",
    "transactions_df['BLOCK_TIMESTAMP'] = pd.to_datetime(transactions_df['BLOCK_TIMESTAMP'])\n",
    "\n",
    "train_df = transactions_df[\n",
    "    (transactions_df['BLOCK_TIMESTAMP'] >= train_start_date) &\n",
    "    (transactions_df['BLOCK_TIMESTAMP'] <= train_end_date)\n",
    "][['BLOCK_TIMESTAMP', 'TX_SENDER_ADDRESS', 'BUY_TOKEN_ADDRESS']]\n",
    "\n",
    "val_df = transactions_df[\n",
    "    (transactions_df['BLOCK_TIMESTAMP'] > train_end_date) &\n",
    "    (transactions_df['BLOCK_TIMESTAMP'] <= val_end_date)\n",
    "][['BLOCK_TIMESTAMP', 'TX_SENDER_ADDRESS', 'BUY_TOKEN_ADDRESS']]\n",
    "\n",
    "test_df = transactions_df[\n",
    "    (transactions_df['BLOCK_TIMESTAMP'] > val_end_date)\n",
    "][['BLOCK_TIMESTAMP', 'TX_SENDER_ADDRESS', 'BUY_TOKEN_ADDRESS']]\n",
    "\n",
    "# note that for link prediction tasks we expect that the destination\n",
    "# entities - the entities that we are trying to predict a link to - \n",
    "# to be grouped in a list. So we will transform our trian,test and\n",
    "# validation dataframes accordingly\n",
    "train_df = train_df.groupby(['TX_SENDER_ADDRESS', 'BLOCK_TIMESTAMP'])['BUY_TOKEN_ADDRESS'].agg(list).reset_index()\n",
    "val_df = val_df.groupby(['TX_SENDER_ADDRESS', 'BLOCK_TIMESTAMP'])['BUY_TOKEN_ADDRESS'].agg(list).reset_index()\n",
    "test_df = test_df.groupby(['TX_SENDER_ADDRESS', 'BLOCK_TIMESTAMP'])['BUY_TOKEN_ADDRESS'].agg(list).reset_index()\n",
    "\n",
    "\n",
    "print(f'Train size: {train_df.shape[0]}')\n",
    "print(f'Validation size: {val_df.shape[0]}')\n",
    "print(f'Test size: {test_df.shape[0]}')\n",
    "\n",
    "# in link prediction problems, validation and test data need\n",
    "# to all have the same timestamp, so we will fake it for now\n",
    "val_df['BLOCK_TIMESTAMP'] =  pd.Timestamp(\"2025-01-31 13:23:59.000\")\n",
    "test_df['BLOCK_TIMESTAMP'] = pd.Timestamp(\"2025-02-01 13:23:59.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d19de",
   "metadata": {},
   "source": [
    "## Upload Data To Snowflake\n",
    "\n",
    "We assume you have created a `.env` file in the same directory as this notebook. (A sample `.env` file is included for reference.)\n",
    "\n",
    "Your `.env` file should contain the following fields:\n",
    "\n",
    "```\n",
    "ACCOUNT_NAME=snowflake_account_name\n",
    "USER_NAME=snowflake_user_name\n",
    "PASSWORD=snowflake_user_password\n",
    "WAREHOUSE=snowflake_warehouse\n",
    "APP_NAME=RAI_EXPT_APP\n",
    "AUTH_METHOD=password\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0769225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_to_snowflake import create_session, load_to_snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520af4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "snowflake_config = {\n",
    "    \"account\": os.getenv(\"ACCOUNT_NAME\"),\n",
    "    \"user\": os.getenv(\"USER_NAME\"),\n",
    "    \"password\": os.getenv(\"PASSWORD\"),\n",
    "    \"warehouse\": os.getenv(\"WAREHOUSE\"),\n",
    "}\n",
    "\n",
    "session = create_session(snowflake_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed235b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_to_snowflake(\n",
    "    session = session,\n",
    "    df = buyers_df,\n",
    "    database=\"GNN_TOKENFLOW\",\n",
    "    schema=\"DATA\",\n",
    "    table_name=\"BUYERS\"\n",
    ")\n",
    "\n",
    "load_to_snowflake(\n",
    "    session = session,\n",
    "    df = senders_df,\n",
    "    database=\"GNN_TOKENFLOW\",\n",
    "    schema=\"DATA\",\n",
    "    table_name=\"SENDERS\"\n",
    ")\n",
    "\n",
    "load_to_snowflake(\n",
    "    session = session,\n",
    "    df = transactions_df,\n",
    "    database=\"GNN_TOKENFLOW\",\n",
    "    schema=\"DATA\",\n",
    "    table_name=\"TRANSACTIONS\"\n",
    ")\n",
    "\n",
    "load_to_snowflake(\n",
    "    session = session,\n",
    "    df = train_df,\n",
    "    database=\"GNN_TOKENFLOW\",\n",
    "    schema=\"TASK\",\n",
    "    table_name=\"TRAIN\"\n",
    ")\n",
    "\n",
    "load_to_snowflake(\n",
    "    session = session,\n",
    "    df = test_df,\n",
    "    database=\"GNN_TOKENFLOW\",\n",
    "    schema=\"TASK\",\n",
    "    table_name=\"TEST\"\n",
    ")\n",
    "\n",
    "load_to_snowflake(\n",
    "    session = session,\n",
    "    df = val_df,\n",
    "    database=\"GNN_TOKENFLOW\",\n",
    "    schema=\"TASK\",\n",
    "    table_name=\"VALIDATION\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18613fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rai_gnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
