{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "lpvwnhqpfzxy346ila3g",
   "authorId": "1343160363561",
   "authorName": "TOKENFLOW_ADMIN",
   "authorEmail": "",
   "sessionId": "98181dd1-0d18-4bee-9000-bbe0f260de94",
   "lastEditTime": 1751042815188
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2ddc4f-a30d-4b8c-aca5-89f0b451a922",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "# Link prediction example using the GNN learning engine under the RelationalAI Snowflake Native App"
  },
  {
   "cell_type": "markdown",
   "id": "5a77adee-f64f-49b4-ab2e-3b931aca698d",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "## ‚úÖ Prerequisites\n\nBefore running this notebook, make sure you've completed the following setup steps:\n\n1. **Install the RelationalAI Experimental Application**\n   Ensure that the RelationalAI experimental app is installed in your Snowflake account.\n\n2. **Preprocess the Tokenflow Dataset**\n   Run the `preprocess_tokenflow_data.ipynb` notebook to generate the processed dataset required for this tutorial.\n\n3. **Upload the Preprocessed Data to Snowflake**\n   Use the preprocessing notebook to upload the Tokenflow data to the appropriate tables in your Snowflake environment.\n\n4. **Install the RelationalAI GNN Python SDK**\n   Follow the installation steps provided in the [README.md](GNN_SDK_DOCUMENTATION.md) to set up the RelationalAI GNN SDK in your environment."
  },
  {
   "cell_type": "markdown",
   "id": "c5fa8ffe-a79f-4d49-8ce8-b3bef8cddf1d",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "### üîê Connecting to the RelationalAI Native App via Snowflake\n\nTo connect to the RelationalAI native app, we'll first set up a dictionary that defines the necessary environment variables for establishing a Snowflake connection. Here we will use\nthe `active_session` authentication method which gets all the credentials needed from the\ncurrent snowflake session. We only need to provide the application name (`RELATIONALAI`).\n\n#### üîê About Authentication\n\nIn this tutorial, we‚Äôll use **active_session**. However, other authentication methods‚Äîsuch as **Key Pair Authentication**, **Password** or **OAuth Token Authentication**‚Äîare also supported. For more details, refer to the [RelationalAI GNN documentation](GNN_SDK_DOCUMENTATION.md).\n\n"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# load the snowflake configuration to a python dict\nsnowflake_config = {\n    \"app_name\": \"RELATIONALAI\",\n    \"auth_method\": \"active_session\"\n}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5b6825e5-7f07-450a-abbd-7a4d1cbcb629",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "\n## ‚öôÔ∏è Managing Your GNN Engines\n\nThe `Provider` class in the RelationalAI GNN SDK allows you to manage your GNN engines seamlessly. Below, we walk through common operations you can perform with the `Provider`:\n\n* ‚úÖ Create a new GNN engine\n* üìã List all available engines\n* üîç Check the status of an engine\n* üîÑ Resume a paused engine\n* ‚ùå Delete an engine\n\nEach of these operations can be done with simple method calls, as shown in the following examples.\n"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "source": "from rai_gnns_experimental import ColumnDType\nfrom rai_gnns_experimental import EvaluationMetric\nfrom rai_gnns_experimental import ForeignKey\nfrom rai_gnns_experimental import GNNTable\nfrom rai_gnns_experimental import JobManager\nfrom rai_gnns_experimental import LinkTask\nfrom rai_gnns_experimental import OutputConfig\nfrom rai_gnns_experimental import Provider\nfrom rai_gnns_experimental import SnowflakeConnector\nfrom rai_gnns_experimental import TaskType\nfrom rai_gnns_experimental import Trainer\nfrom rai_gnns_experimental import TrainerConfig\n\n\n\nimport time",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5c7bb107-b465-49aa-9d9b-3adb1ca18042",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# engine setup\nengine_name = \"my_engine\"\nengine_size = \"GPU_NV_S\" # Available sizes: \"GPU_NV_S\" or \"HIGHMEM_X64_S\"\n\n# database /s chema\nDB_NAME = \"TF_DB\"\nSCHEMA_NAME = \"TF_SCHEMA\"\nDB_SCHEMA = DB_NAME+\".\"+SCHEMA_NAME\n\n\n# dataset name\nDATASET_NAME = \"tokenflow\"\n\n# buyer data\nBUYER_SOURCE_TABLE = DB_SCHEMA+\".BUYERS\"\nBUYER_NAME = \"buyers\"\nBUYER_PRIMARY_KEY = \"BUY_TOKEN_ADDRESS\"\n\n# sender data\nSENDER_SOURCE_TABLE = DB_SCHEMA+\".SENDERS\"\nSENDER_NAME = \"senders\"\nSENDER_PRIMARY_KEY = \"TX_SENDER_ADDRESS\"\n\n# transaction data\nTRANSACTION_SOURCE_TABLE = DB_SCHEMA+\".TRANSACTIONS\"\nTRANSACTION_NAME = \"transactions\"\nTIME_COLUMN = \"BLOCK_TIMESTAMP\"\n\n# link task\nLINK_TASK_NAME = \"sender_buyer\"\nTASK_TRAIN_TABLE = DB_SCHEMA+\".TRAIN\"\nTASK_TEST_TABLE  = DB_SCHEMA+\".TEST\"\nTASK_VALIDATION_TABLE = DB_SCHEMA+\".VALIDATION\"\n\n# model params\nMODEL_DEVICE = \"cuda\" # either 'cuda' or 'cpu'\nMODEL_N_EPOCHS = 3\nMODEL_MAX_ITERS = 200\nMODEL_TEXT_EMBEDDER = \"model2vec-potion-base-4M\"\n\n\nOUTPUT_ALIAS = \"TEST_PREDS_1\"\nTEST_BATCH_SIZE = 128",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e45ffda-01b0-4e30-b4e9-8d8900371883",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# initialize the provider using the snowflake configuration\n# (note: you might be prompted from your MFA app at this point)\nprovider = Provider(**snowflake_config)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec16a736-ee1f-4237-8b7f-8844abff6fb8",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# We can create a new GNN engine, currently two\n# engine types are supported \"GPU_NV_S\" and \"HIGHMEM_X64_S\"\n# here we create an engine by specifying the engine name (a\n# custom name) and also the type of the engine, specified by the\n# \"size\" parameter\n# Available sizes: \"GPU_NV_S\" or \"HIGHMEM_X64_S\"\n#\n# provider.create_gnn(\n#    name=engine_name,\n#    size=\"GPU_NV_S\"  # or \"HIGHMEM_X64_S\"\n#)\n\n\n# check if engine exists, if yes, resume if not create it\nif not provider.get_gnn(engine_name): \n    print(f\"Creating Engine {engine_name}\")\n    provider.create_gnn(name=engine_name, size=engine_size)\nelse:\n    print(f\"Resuming Engine {engine_name}\")\n    provider.resume_gnn(name=engine_name)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "777e2537-2d48-46f2-87a6-ee7d82c0b2d1",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# HINT: We can always resume a GNN engine that has been suspended:\n# Note: Engine provisioning can take some minutes. Please\n# check the engine status using provider.get_gnn(name=\"my_engine\")\n\n# provider.resume_gnn(name=engine_name)\n\n# And if we need we can also delete a GNN engine\n\n# provider.delete_gnn(name=engine_name)\n\n# we can also check the existing engines. If there is not engines listed here you would need to create one using provider.create_gnn(name=\"my_engine\", size=\"GPU_NV_S\")\n# provider.list_gnns()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cd534f9-b527-4967-8358-18f317e1dcbf",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n\n# If we resume an engine, we can directly see the status of the engine\n# the status of the engine 'READY' marks the fact that the\n# engine is ready to be used. A `PENDING` status marks the\n# fact that the engine has  been automaticaly suspended.\n# Notice also that under the settings \n# the provider exposes a URL for the MLFLOW endpoint\n# that we can use to track our experiments\n# NOTE: we should wait until the status is READY\n\nengine_data = provider.get_gnn(engine_name) \nwhile not engine_data or engine_data[\"state\"] != 'READY':\n    time.sleep(10)\n    engine_data = provider.get_gnn(engine_name) \n    \n\nprint(f'ENGINE {engine_name} READY')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58197475-31e7-4c07-a93d-5ab62bfc8e36",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "‚ö†Ô∏è **Warning:** To  make sure to check the engine status (e.g., via `provider.get_gnn(\"engine_name\")`) and make sure that the status is `READY` \n"
  },
  {
   "cell_type": "markdown",
   "id": "c507459a-aec9-4fb4-9a10-92a095739ae1",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "## üîå Connector Setup\n\nThe `Connector` class, like the `Provider` class, is used to communicate with Snowflake. However, while the `Provider` is responsible for managing GNN engines, the `Connector` is specifically used to interface with the **GNN learning engine** itself.\n\nYou‚Äôll use the `Connector` instance as an input to all SDK components that need to send requests to the GNN engine‚Äîsuch as loading data, running training jobs, or performing inference.\n\nIn short:\n\n* `Provider` ‚Üí Manages GNN engine instances (create, list, delete, etc.)\n* `Connector` ‚Üí Sends requests to a specific GNN engine for processing tasks\n\nLet‚Äôs now walk through how to create and use a `Connector`.\n"
  },
  {
   "cell_type": "code",
   "id": "5165f427-bbb2-4c09-b846-1f81eca2b7db",
   "metadata": {
    "language": "python",
    "name": "cell14",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# we initialize the connector and passing all our credentials.\nconnector = SnowflakeConnector(\n    **snowflake_config,\n    engine_name=engine_name,\n)\n# the connector also provides access to MLFLOW that you can\n# use to monitor your experiments and register trained GNN models\n# connector.mlflow_session_url",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41019865-c004-44d0-a3cf-aac6e27548d1",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "## üìä Preparing the data: Creating the GNN tables\n\nIn this section, we will define the GNN tables and the associated learning task. These components will then be used to construct a GNN dataset suitable for training.\n\nFor this tutorial, we‚Äôll use the Tokenflow database as our working example. This database includes three tables: BUYERS, SENDERS, and TRANSACTIONS. The TRANSACTIONS table links BUYERS to SENDERS. Our objective is to predict which BUYERS a given SENDER is most likely to transact with in the future‚Äîframing this as a link prediction task.\n\nBefore loading the data, ensure that your application has the necessary permissions to access the relevant tables. In Snowflake, this can be achieved by running the following commands:\n\n```sql\n-- here we grant access to all schemas and tables, you might want to\n-- select specific tables and schemas to grant access to\nGRANT USAGE ON DATABASE GNN_TOKENFLOW TO APPLICATION RELATIONALAI;\nGRANT USAGE ON ALL SCHEMAS IN DATABASE GNN_TOKENFLOW TO APPLICATION RELATIONALAI;\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN DATABASE GNN_TOKENFLOW TO APPLICATION RELATIONALAI;\n-- grant write access to write results, we encourage the user to select specific schemas\n-- to give write access to\nGRANT CREATE TABLE ON ALL SCHEMAS IN DATABASE GNN_TOKENFLOW TO APPLICATION RELATIONALAI;\n```"
  },
  {
   "cell_type": "code",
   "id": "26e6efaa-0947-4672-b52e-c5b39e94c9a5",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "# create a table for the buyers and set the \n# BUY_TOKEN_ADDRESS as a primary key (primary and \n# foreign keys are used to construct the edges of the graph)\nbuyers_table = GNNTable(\n    connector=connector,\n    source=BUYER_SOURCE_TABLE,\n    name=BUYER_NAME,\n    primary_key=BUYER_PRIMARY_KEY,\n)\nbuyers_table.show_table()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "959bcd1f-1a9b-4514-bf05-5f2f3cb82aa4",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# in a similar manner we can create the SENDERS table\nsenders_table = GNNTable(\n    connector=connector,\n    source=SENDER_SOURCE_TABLE,\n    name=SENDER_NAME,\n    primary_key=SENDER_PRIMARY_KEY,\n)\nsenders_table.show_table()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4f4a25b-7f89-496b-b6bc-721316ab7443",
   "metadata": {
    "language": "python",
    "name": "cell18",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# and finally we will link the two tables using the foreign\n# keys from the TRANSACTIONS table. Note: the transactions\n# table  has also one special \"time column\" that will be used\n# to prevent data leakage (see the documentation for more details)\ntransactions_table = GNNTable(\n    connector=connector,\n    source=TRANSACTION_SOURCE_TABLE,\n    name=TRANSACTION_NAME,\n    foreign_keys=[\n        ForeignKey(\n            column_name=SENDER_PRIMARY_KEY, link_to=SENDER_NAME+\".\"+SENDER_PRIMARY_KEY\n        ),\n        ForeignKey(column_name=BUYER_PRIMARY_KEY, link_to=BUYER_NAME+\".\"+BUYER_PRIMARY_KEY),\n    ],\n    time_column=TIME_COLUMN,\n)\n# modify column dtypes if needed\ntransactions_table.update_column_dtype(\n    col_name=\"BUY_AMOUNT\", dtype=ColumnDType.integer_t\n)\ntransactions_table.update_column_dtype(\n    col_name=\"SELL_AMOUNT\", dtype=ColumnDType.integer_t\n)\ntransactions_table.show_table()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2f28ed2-8e34-4ce6-b7d9-25092469a16e",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "\n## üîß Preparing the Data: Creating the Task\n\nTo define the task, we begin by specifying the locations of the training, validation, and test datasets. We also identify the source and destination entity tables, along with the corresponding columns that uniquely identify each entity.\n\nSince this is a **link prediction** task, our objective is to predict future connections between a source entity and a destination entity.\n\nAdditionally, we define a **timestamp column** to avoid information leakage by ensuring that future data doesn't influence past predictions (see [documentation](GNN_SDK_DOCUMENTATION.md) for details). Lastly, we specify the **evaluation metric**‚Äîin this case, **Mean Average Precision (MAP)**‚Äîto assess the performance of the model.\n"
  },
  {
   "cell_type": "code",
   "id": "43bc319e-6c98-464f-9f07-3479366ce8ab",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "link_pred_task = LinkTask(\n    connector=connector,\n    name=LINK_TASK_NAME,\n    task_data_source={\n        \"train\": TASK_TRAIN_TABLE, \n        \"test\": TASK_TEST_TABLE, \n        \"validation\": TASK_VALIDATION_TABLE\n    },\n    # name of source entity column that we want to do predictions for\n    source_entity_column=SENDER_PRIMARY_KEY,\n    # name of GNN table that column is at\n    source_entity_table=SENDER_NAME,\n    # name of target entity column that we want to predict\n    target_entity_column=BUYER_PRIMARY_KEY,\n    # name of GNN table that column is at\n    target_entity_table=BUYER_NAME,\n    time_column=TIME_COLUMN,\n    task_type=TaskType.LINK_PREDICTION,\n    evaluation_metric=EvaluationMetric(name=\"link_prediction_map\", eval_at_k=12),\n)\n\nlink_pred_task.show_task()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6420d9ed-dfd3-4231-8da5-3a9304eb4f12",
   "metadata": {
    "name": "cell21"
   },
   "source": "## üß© Preparing the Data: Creating the Dataset\n\nFinally, we combine all the components by constructing a dataset object that encapsulates both the GNN tables and the task definition. This dataset will serve as the input to the model training pipeline, ensuring that the task and its associated data are tightly integrated and ready for downstream processing.\n"
  },
  {
   "cell_type": "code",
   "id": "ede201c5-3ae4-466d-829a-83315dfdc67e",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "from rai_gnns_experimental import Dataset\n\ndataset = Dataset(\n    connector=connector,\n    dataset_name=DATASET_NAME,\n    tables=[buyers_table, transactions_table, senders_table],\n    task_description=link_pred_task,\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10798e21-6006-4933-85e1-443eddd30c34",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": "# we can also visualize the dataset \nfrom graphviz import Source\ngraph = dataset.visualize_dataset(show_dtypes=True)\n# play with font size and plot size to get a good visualization\nfor node in graph.get_nodes():    \n    font_size = node.get_attributes()['fontsize']\n    font_size = \"16\"\n    node.set('fontsize', font_size)\n    \n\ngraph.set_graph_defaults(size='\"10,10!\"')\n\nsrc = Source(graph.to_string())\nsrc  # Display in notebo",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "68a6d0a1-b3c5-485e-837a-38ccb44cc30a",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "## üöÄ GNN Model Training\n\nNow that our dataset is ready, we can train our first GNN model. We‚Äôll begin by defining a **configuration** that specifies the training parameters, such as model architecture, optimizer settings, and training duration.\n\nNext, we‚Äôll instantiate a **trainer** using this configuration. The trainer will consume the dataset we previously created and manage the entire training process. By calling the `fit()` method on the trainer, we initiate a training job‚Äîwhose progress and status can be monitored throughout execution.\n\n"
  },
  {
   "cell_type": "code",
   "id": "ff2219a5-e80e-4079-9b3c-de6d08afdc3c",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": "# the first step will be to define a configuration for our Trainer.\n# the configuration includes many parameters that are explained in\n# detail in the documentation. It does not only provide parameters\n# for the graph neural network but also parameters for other components\n# of the model (such as feature extractors, prediction head parameters,\n# training parameters etc.)\nmodel_config = TrainerConfig(\n    connector=connector,\n    device=MODEL_DEVICE,  # either 'cuda' or 'cpu'\n    n_epochs=MODEL_N_EPOCHS,\n    max_iters=MODEL_MAX_ITERS,\n    text_embedder=MODEL_TEXT_EMBEDDER,\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "873cb178-9c7f-4ca7-b38c-d0109aa124c5",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "# we initialize now our trainer object with the trainer configuration\n# the trainer object can be used to train a model, to perform inference\n# or to perform training & inference.\ntrainer = Trainer(connector=connector, config=model_config)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eac77106-4c4d-45ab-8aa5-f1232f0f4d05",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "# in our first example we will use the trainer to perform training only.\n# every time the trainer is \"executed\" (calling fit(), predict() or fit_predict())\n# it returns a job object that can be used to monitor the current running job.\n# See the documentation for the meaning of the job statuses\ntrain_job = trainer.fit(dataset=dataset)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0393c3a6-8b27-4bb1-b680-8351dd28cd70",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "# now we can monitor the job status\n# observe that once the job is running we also get back an experiment name\n# we will see later how we can use that to perform inference\ntrain_job.get_status()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9432ca5e-9a1e-4c8a-a9bf-56da1996a2f3",
   "metadata": {
    "language": "python",
    "name": "cell29",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# we can also stream the logs of the training job in real time\n# Hint: You can stop the cell execution to stop monitoring of logs\n# Hint: At this point you can also open the MLFLow URL to monitor your experiments\ntrain_job.stream_logs()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b91438c9-6e3b-4674-899c-c2ee399ff6aa",
   "metadata": {
    "language": "python",
    "name": "cell30",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# HINT: one can cancel a running job as well\n# train_job.cancel()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe518e6b-550b-4f4f-83e9-7d02f4ef4757",
   "metadata": {
    "language": "python",
    "name": "cell31",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# it might be the case that we have lost track of\n# the jobs that we are running. To this end we also\n# provide to the user a JobManager object that can\n# give us the status of all current jobs. Let's see\n# an example:\njob_manager = JobManager(connector=connector)\njob_manager.show_jobs()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcd6ce86-43a8-47fe-a8dd-766af7f3075a",
   "metadata": {
    "language": "python",
    "name": "cell32",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# we can also get back job details using it's job id\n# foo_job = job_manager.fetch_job(\"01bd4f74-0000-21eb-001d-cf8b00125bc2\")\n# hint: the job manager can be used to cancel any job as well",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d89578c-3e1e-4021-aada-4621b1e04cfa",
   "metadata": {
    "name": "cell33",
    "collapsed": false
   },
   "source": "## üîç Inference Using a Trained Model\n\nFinally, we‚Äôll demonstrate how to perform inference using the model we‚Äôve just trained. In this example, we'll directly use the recently trained model to generate predictions.\n\nFor more advanced use cases‚Äîsuch as registering a model for reuse or automatically selecting the best-performing model‚Äîplease refer to the [documentation](GNN_SDK_DOCUMENTATION.md) for further guidance.\n"
  },
  {
   "cell_type": "code",
   "id": "fb3f1703-4536-49bf-9fef-bc7e40e318aa",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": "output_config = OutputConfig.snowflake(database_name=DB_NAME, schema_name=SCHEMA_NAME)\n# make sure that the table with the same alias does not already exist\n# we never overwrite tables\ninference_job = trainer.predict(\n    output_alias=OUTPUT_ALIAS,\n    output_config=output_config,\n    test_batch_size=TEST_BATCH_SIZE,\n    dataset=dataset,\n    model_run_id=train_job.model_run_id,\n    extract_embeddings=True,\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a7b8e0e-a4bb-40dc-9a84-214ac333c5d9",
   "metadata": {
    "language": "python",
    "name": "cell35",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "inference_job.get_status()",
   "execution_count": null
  }
 ]
}