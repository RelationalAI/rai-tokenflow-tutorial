{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "lastEditStatus": {
   "notebookId": "snnhx4ruo36xzsjvyt64",
   "authorId": "7316701521544",
   "authorName": "ALEX_NTOULAS",
   "authorEmail": "alex.ntoulas@relational.ai",
   "sessionId": "3c0490f6-1132-41e4-9290-00fc9f45cd95",
   "lastEditTime": 1751068343959
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f17c12-123f-4048-90c5-b2cba013cfd5",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# Tokenflow: Running the RAI GRS Pipeline on Agents Data\n\n## Notebook-Based Pipeline Execution\nThis notebook demonstrates running the pipeline by using Python Snowflake Connector calls. The notebook involves:\n- Configuration Loading: Create a configuration for your project\n- Pipeline Invocation: Execute corresponding pipeline steps using direct calls throught the Python Snowflake Connector.\n- Visualization: Generate and display the Knowledge Graph visualization.\n\n**Note:** In this notebook, we demonstrate only the basic pipeline execution. However, the application also supports fine-tuning one of the available Cortex LLMs on your own documents. For the purposes of this demo, we use the default LLMs for both the extraction and question answering steps.\n\n**Alternative Workflow:** If you have already ran the pipeline through the UI, you can use this notebook by skipping the initial sections related to pipeline execution. Instead, go directly to the *'Load extracted graph data from Snowflake'* section. This allows you to proceed straight to the visualization phase."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759050f-aafd-4605-993f-256563453142",
   "metadata": {
    "name": "cell2",
    "language": "python"
   },
   "outputs": [],
   "source": "import gravis as gv\nimport json\nimport networkx as nx\nimport snowflake.connector\n\nfrom datetime import datetime, timezone\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.connector.pandas_tools import write_pandas"
  },
  {
   "cell_type": "markdown",
   "id": "a8839037-5085-46a1-afcb-b7b6758f66fd",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "## Create your project"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2641a9b-83e5-479e-b77b-6b4e153337c1",
   "metadata": {
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": "# app name\nAPP_NAME = \"RAI_GRS\"\n\nDB_NAME = \"TF_DB\"\nSCHEMA_NAME = \"TF_SCHEMA\"\nDATA_TABLE_NAME = \"VIRTUALS_AGENTS\"\n\n# Set up our project names\nPROJECT_ID = 'tokenflow123'\nPROJECT_NAME = PROJECT_ID\nPROJECT_COMMENTS = f\"Project deployment for {PROJECT_ID}\"   # add more comments here\n\n# data\nDATA_SOURCE = 'CORPUS' # 'CORPUS' if your run the pipeline for the Tokenflow use case, or 'PDF' if you have uploaded files to the FILES stage.\n\n# LLM setup\nLLM_FAMILY = 'snowflake'    # 'openai' or 'snowflake'\nOPENAI_API_KEY = None       # if you picked 'openai' as llm_family please insert your key here\nCOMPLETION_MODEL = \"llama3.3-70b\"   # For OpenAI: \"gpt-4.1\" or \"gpt-4o\". For Snowflake Cortex: \"claude-3-5-sonnet\" or \"llama3.3-70b\".\nEMBEDDINGS_MODEL = \"e5-base-v2\"     # For OpenAI: \"text-embedding-3-small\" and \"text-embedding-3-large\". For Snowflake Cortex: \"snowflake-arctic-embed-l-v2.0\" and \"e5-base-v2\".\nIS_FINE_TUNED_COMPLETION_MODEL = False  # we don't use fine tuning in this notebook [default]\nSUMMARIZATION_CONTEXT =\"chunk\"          # we use chunks for summaries [default]\n\n# community detection tuning\nCOMMUNITY_DETECTION_PARAMS = {\n    \"algorithm\": \"leiden\",\n    \"initial_membership\": None,\n    \"weights\": None,\n    \"n_iterations\": 2,\n    \"max_comm_size\": 0,\n    \"seed\": None\n}\nCOMMUNITY_DETECTION_APPEND = False",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f3ce4-addb-4ea6-bcb2-aa396cf39525",
   "metadata": {
    "name": "cell3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "conn = snowflake.connector.connect(session=session)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1077f-5e82-4a69-a786-ab7ccbbe8187",
   "metadata": {
    "name": "cell6",
    "language": "python"
   },
   "outputs": [],
   "source": "# If this is a new project, we need to insert it as a new record into the PROJECTS table.\n# All changes made using the cursor are automatically saved to the SF cloud database.\n# Define the SQL query with the NOT EXISTS check.\n\n# We will use the default config and provide parameters in some of the functions here\nextraction_call = f\"\"\"\n    INSERT INTO {APP_NAME}.DATA.PROJECTS (ID, NAME, CONFIG, COMMENTS)\n    SELECT '{PROJECT_ID}', '{PROJECT_NAME}', NULL, '{PROJECT_COMMENTS}'\n    WHERE NOT EXISTS (\n        SELECT 1 \n        FROM {APP_NAME}.DATA.PROJECTS \n        WHERE ID = '{PROJECT_ID}'\n    )\n\"\"\"\n\n# Execute the SQL call, providing the values for the placeholders.\ncursor.execute(extraction_call)\n\n# Fetch and print all rows returned by the Snowflake query executed via the cursor\nresults = cursor.fetchall() \nfor row in results:\n    print(row)"
  },
  {
   "cell_type": "markdown",
   "id": "5bc9936d-b5b7-4bc3-9c06-03796ed4b779",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e306863-6a58-4b80-a282-f3f033bf9bf8",
   "metadata": {
    "name": "cell13"
   },
   "source": [
    "#### 0. Corpus conversion\n",
    "The very first step is to upload your documents to the `FILES` stage (under the schema `RAI_GRS.DATA`) in a new folder named after the `project_id`. Once uploaded, you can run the corpus conversion, which extracts text content from your documents.\n",
    "\n",
    "There are two options available: standard conversion and visual parsing. Visual parsing leverages an LLM with vision capabilities to interpret and extract meaning from visual content in your documents, such as images and diagrams.\n",
    "\n",
    "For this example, we already have a table with textual data, so there are no PDF documents to process through the corpus conversion endpoint. Instead, we take the relevant textual column from the input file, and format it to match the structure of the `CORPUS` table, as it would appear after running the conversion on PDF documents (i.e., with the same columns and structure). \n",
    "\n",
    "Once formatted, we upload this data to Snowflake so it is ready for the first step of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d79ed-ce37-4961-b4f9-62c1ef039b0d",
   "metadata": {
    "name": "cell14",
    "language": "python"
   },
   "outputs": [],
   "source": "# Step 0\n# -- Corpus conversion (e.g. PDF to MarkDown).\n\n\nif DATA_SOURCE == 'pdf':\n    # Form the SQL call for the execute_convert_corpus using the default parameters.\n    extraction_call = f\"\"\"CALL {APP_NAME}.app.execute_convert_corpus('{PROJECT_ID}');\"\"\"\n    \n    # Or if the documents contain visual content, we could run this process (note that it may be costly due to extensive LLM calls): \n    # extraction_call = f\"\"\"CALL RAI_GRS.app.execute_llm_convert_corpus('{project_id}');\"\"\"\n    \n    # Execute the SQL call.\n    cursor.execute(extraction_call)\n    \n    # See the results of the call.\n    results = cursor.fetchall() \n    for row in results:\n        print(row)\nelse:\n    # Corpus convertion no needed here as we already have the raw text in a CSV file, so we will provide the corpus table.\n    # We read the table of interest that has the textual data. \n    sql = f\"SELECT * FROM {DB_NAME}.{SCHEMA_NAME}.{DATA_TABLE_NAME};\"\n    cursor.execute(sql)\n    \n    # Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n    virtual_agents_raw = cursor.fetch_pandas_all()\n    # In this experiment, we extract the name, symbol, and description of the agents, and combine them into a single string.\n    # This aggregated text will serve as the input document (one for each agent) for our algorithm.\n    corpus = virtual_agents_raw[[\"NAME\", \"SYMBOL\", \"DESCRIPTION\"]].copy()\n    \n    # Create the necessary columns to match the schema of the Snowflake corpus table.\n    corpus[\"PROJECT_ID\"] = PROJECT_ID\n    corpus[\"CHUNK_ID\"] = corpus[\"NAME\"].apply(lambda x: f\"{PROJECT_ID}/{x}\")\n    \n    # We could have custom metadata for the different documents, or let the LLM generate some (e.g. short title) but for now we use the same \n    # metadata for all the records. Note that if the \n    metadata_for_all_entries = {\n      \"creationDate\": datetime.now(timezone.utc).strftime(\"D:%Y%m%d%H%M%SZ\"),\n      \"subject\": \"Text with descriptions of the agents.\",\n      \"source\": DATA_TABLE_NAME\n    }\n    corpus[\"METADATA\"] = corpus.apply(lambda _: metadata_for_all_entries, axis=1)\n    corpus[\"CONTENT\"] = corpus.apply(lambda row: f\"Agent with name {row['NAME']} has symbol: {row['SYMBOL']} and description: {row['DESCRIPTION']}.\", axis=1)\n    \n    # The final table format is matches the CORPUS table schema on Snowflake.\n    final_corpus_df = corpus[[\"PROJECT_ID\", \"CHUNK_ID\", \"CONTENT\", \"METADATA\"]]\n    # Now that we have ready the textual data into the corpus table, we upload it to Snowflake\n    # Then we can run the GraphRAG Native App pipeline.\n    \n    # Upload corpus table to snowflake usint write_pandas from Snowflake API.\n    success, nchunks, nrows, _ = write_pandas(conn=conn,\n                                              df=final_corpus_df,\n                                              database=f'{APP_NAME}',\n                                              schema='DATA',\n                                              table_name='CORPUS')\n    \n# show some sample data of what we created\ndf = session.sql(f\"SELECT * FROM {APP_NAME}.DATA.CORPUS WHERE PROJECT_ID = '{PROJECT_ID}' LIMIT 100;\") \ndf.collect()"
  },
  {
   "cell_type": "markdown",
   "id": "c36de125-0001-4995-b748-cc14911e7b0a",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "#### 1. Entities and relations extraction \n",
    "\n",
    "After preparing the text documents, we can begin our pipeline with the first actual step of the knowledge graph (KG) construction. \n",
    "We are using the default prompt that comes with the installation of the app‚Äîit has been written to perform well across a variety of document types and domains. This process may take some time, depending on the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df9004-7bb5-48c7-9e84-d3895c92b02a",
   "metadata": {
    "name": "cell16",
    "language": "python"
   },
   "outputs": [],
   "source": "# Step 1\n# -- Entities / relations extraction.\n\n# Form the SQL call.\nextraction_call = f\"\"\"CALL {APP_NAME}.app.execute_get_entities_relations(   '{PROJECT_ID}',\n                                                                            '{LLM_FAMILY}',\n                                                                            '{COMPLETION_MODEL}',\n                                                                            '{IS_FINE_TUNED_COMPLETION_MODEL}',\n                                                                            '{OPENAI_API_KEY}');\"\"\"\n                                                                             \n\n# Execute the SQL call.\ncursor.execute(extraction_call)\n\n# See the results of the call.\nresults = cursor.fetchall() \nfor row in results:\n    print(row)"
  },
  {
   "cell_type": "markdown",
   "id": "0fd38ad3-9a51-4233-bead-5cc0ab4dc493",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "#### 2. Community detection\n",
    "\n",
    "After extracting entities and relations, the next step is community detection. Several algorithms are available for this task, with configurable parameters. For example, you can set a maximum community size to prevent the formation of overly large communities with too many nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7848a-27bf-4e20-b149-dcd434acf737",
   "metadata": {
    "name": "cell18",
    "language": "python"
   },
   "outputs": [],
   "source": "# Step 2\n# -- Community detection.\n\n# Convert parameters dict to JSON string if needed\nparams_json = json.dumps(COMMUNITY_DETECTION_PARAMS)\nextraction_call = f\"\"\"CALL {APP_NAME}.app.execute_get_communities( '{PROJECT_ID}',\n                                                                PARSE_JSON('{params_json}'),\n                                                                {str(COMMUNITY_DETECTION_APPEND).upper()});\"\"\"\n\n# Execute the SQL call.\ncursor.execute(extraction_call)\n\n# See the results of the call.\nresults = cursor.fetchall() \nfor row in results:\n    print(row)"
  },
  {
   "cell_type": "markdown",
   "id": "a2097132-8fb4-453e-9de3-cf9d103e3b61",
   "metadata": {
    "name": "cell19"
   },
   "source": [
    "#### 3. Graph indexing: summarization and embeddings\n",
    "\n",
    "This step performs the following operations:\n",
    "\n",
    "- Summarization of each community with LLM, capturing the context of the nodes it contains  \n",
    "- Embedding generation for the `CORPUS` table, node and edge properties, and the community summaries\n",
    "\n",
    "As with any LLM task of your pipeline, you can adjust the summarization prompt to guide the LLM on the desired level of abstraction and which details to include in the summaries. In this case, we use the default prompt. Note that this step is time-consuming, as it involves two separate processes with multiple AI model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159817f-9564-4d32-94ce-5c410dbfbfb0",
   "metadata": {
    "name": "cell20",
    "language": "python"
   },
   "outputs": [],
   "source": "# Step 3\n# -- Graph indexing: summarization and embeddings.\n\n# Form the SQL call.\nextraction_call = f\"\"\"CALL {APP_NAME}.app.execute_get_embeddings(  '{PROJECT_ID}',\n                                                                '{LLM_FAMILY}',\n                                                                '{COMPLETION_MODEL}',\n                                                                '{EMBEDDINGS_MODEL}',\n                                                                '{SUMMARIZATION_CONTEXT}',\n                                                                '{IS_FINE_TUNED_COMPLETION_MODEL}',\n                                                                '{OPENAI_API_KEY}');\"\"\"\n\n\n# Execute the SQL call.\ncursor.execute(extraction_call)\n\n# See the results of the call.\nresults = cursor.fetchall() \nfor row in results:\n    print(row)"
  },
  {
   "cell_type": "markdown",
   "id": "23340232-d6a8-408f-ac33-899932443209",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "#### 4. Question Answering (QA)\n",
    "\n",
    "After indexing completes, we are ready to use our app for question answering!\n",
    "\n",
    "Note that the indexing phase occurs not only during embedding generation with your selected embedder but also within the Cortex Search service.\n",
    "\n",
    "This provides two retrieval options:\n",
    "- **Vector search**, which uses the embeddings generated in the previous step (step #3)  \n",
    "- **Cortex Search**, the managed service provided by Snowflake Cortex, which performs hybrid retrieval and reranking behind the scenes\n",
    "\n",
    "You can also select which sources to include in the retrieved results, since there are multiple content types to search: corpus items, community summaries, and verbalized properties of both nodes and edges. For now, we use the default behavior, which searches the most relevant across all available sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984be40-0084-4f04-95e1-9fb41df020bc",
   "metadata": {
    "scrolled": true,
    "name": "cell22",
    "language": "python"
   },
   "outputs": [],
   "source": "# Step 4\n\n# Form the SQL call.\n# question = \"What is the meaning of the context?\"\nquestion = \"What do you know about WAI Combinator?\"\n\nextraction_call = f\"\"\"CALL {APP_NAME}.app.execute_get_answer('{PROJECT_ID}', '{question}');\"\"\"\n\n# Execute the SQL call.\ncursor.execute(extraction_call)\n\n# See the results of the call.\nresult = cursor.fetchone() \nanswer = result[1]\ncontext = json.loads(result[2])\nprint(f\"Question: {question}\")\nprint()\nprint(f\"Answer: {answer}\")\nprint()\nprint(\"-----------------------------------\")\nprint(f\"Context has {len(context)} items.\")"
  },
  {
   "cell_type": "markdown",
   "id": "e03626b8-ffd5-493c-9c39-38637129004a",
   "metadata": {
    "name": "cell23"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "#### Load extracted graph data from snowflake \n",
    "\n",
    "The pipeline run produces graph data such as nodes, edges, and their properties, and stores them in Snowflake tables. After running the pipeline, we can download this graph data for use in other applications or purposes. In this case, we get back the nodes and edges to create a simple visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c55c9-9e44-40c4-8e84-2b61170c5483",
   "metadata": {
    "name": "cell24",
    "language": "python"
   },
   "outputs": [],
   "source": "# We use the provided method from Snowflake Python connector to get the graph data:\n# https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#fetch_pandas_all\n\ncursor.execute(f\"SELECT * FROM {APP_NAME}.DATA.NODES WHERE PROJECT_ID='{PROJECT_ID}'\")\n# Fetch the result set from the cursor and deliver it as the pandas DataFrame.\nnodes = cursor.fetch_pandas_all()\nnodes.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a42ab-9830-48de-a4d7-739bdaec6f58",
   "metadata": {
    "name": "cell25",
    "language": "python"
   },
   "outputs": [],
   "source": "cursor.execute(f\"SELECT * FROM {APP_NAME}.DATA.EDGES WHERE PROJECT_ID='{PROJECT_ID}'\")\nedges = cursor.fetch_pandas_all()\nedges.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5e41e-3362-423c-bc47-1a0abf76b5bb",
   "metadata": {
    "name": "cell26",
    "language": "python"
   },
   "outputs": [],
   "source": "#conn.close()\n#cursor.close()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00393b-98bd-497f-bc79-7525cffbd9b4",
   "metadata": {
    "name": "cell27",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def get_node_style(node_type):\n",
    "    \"\"\"Get appropriate icon and color based on node type with grouped synonyms.\"\"\"\n",
    "    \n",
    "    grouped_styles = {\n",
    "        # You need to make sure every key in grouped_styles is a tuple, even if it only has one item. In Python, \n",
    "        # that means you must include a trailing comma e.g. ('employer',): ('üë§üíº', '#FFB6C1'),:\n",
    "        ('ai_agent', 'agent', 'ai', 'ai_technology', 'ai_framework',): ('ü§ñ', '#FFB6C1'),\n",
    "        ('person', 'user', 'family_member',): ('üßë', '#DAA06D'), \n",
    "        ('platform', 'software', 'technology', 'feature',): ('üñ•Ô∏è', '#98FB98'),\n",
    "        ('blockchain', 'cryptocurrency', 'trading_platform', 'token', 'blockchain_paradise', 'meme_coin',): ('‚Çø', '#4682B4'),\n",
    "        ('ai_agent_role',): ('ü¶æ', '#FFD700'),\n",
    "        ('financial_product', 'currency',): ('üí∞', '#FFA500'),\n",
    "        ('company', 'organization',): ('üè¢', '#90EE90'),\n",
    "        ('document',): ('üìÑ', '#ADD8E6'),\n",
    "        ('legal_code', 'law', 'legislation', 'legal_document', 'legal_act'): ('‚öñÔ∏è', '#B0C4DE'), \n",
    "        ('country', 'place', 'ecosystem', 'location', 'territory',): ('üåç', '#DDA0DD'),\n",
    "        ('certification',): ('üéñÔ∏è', '#DC143C'),\n",
    "        ('regulation', 'legal', 'protocol', 'algorithm',): ('üìú', '#DDA0DD'),\n",
    "        ('default',): ('üìå', '#F0F0F0')\n",
    "    }\n",
    "\n",
    "    # Flatten into a usable dictionary\n",
    "    styles = {\n",
    "        synonym.lower(): style\n",
    "        for keys, style in grouped_styles.items()\n",
    "        for synonym in keys\n",
    "    }\n",
    "\n",
    "    return styles.get(node_type.lower(), styles['default'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e90e6-3f81-4a4d-ace5-b22761d9b406",
   "metadata": {
    "name": "cell28",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Filter the nodes DataFrame to include only those nodes that are part of any edge (either as a source or destination in the edges DataFrame).\n",
    "\n",
    "# Step 1: Get all node IDs involved in edges (both source and destination)\n",
    "edge_node_ids = set(edges['SRC_NODE_ID']).union(set(edges['DST_NODE_ID']))\n",
    "\n",
    "# Step 2: Filter the nodes DataFrame to keep only rows where ID is in edge_node_ids\n",
    "filtered_nodes = nodes[nodes['ID'].isin(edge_node_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f8c56-898b-4add-b2d7-85ca28fce3d1",
   "metadata": {
    "name": "cell29",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "for index, row in filtered_nodes.iterrows(): \n",
    "    node_id = row['ID']\n",
    "    node_type = row['TYPE']\n",
    "    entity_emoji, entity_color = get_node_style(node_type=row['TYPE']) \n",
    "    entity_label = f\"{entity_emoji} {row['ID']}\"\n",
    "    G.add_node(node_id, \n",
    "               label=entity_label,\n",
    "           type=node_type,\n",
    "           color=entity_color,\n",
    "           size=20)\n",
    "\n",
    "\n",
    "for index, row in edges.iterrows():        \n",
    "    G.add_edge(row['SRC_NODE_ID'], row['DST_NODE_ID'], label=row['TYPE'], size=1, length=500)\n",
    "\n",
    "graph = gv.d3(\n",
    "    G,\n",
    "    node_label_data_source='label', \n",
    "    show_edge_label=True, \n",
    "    edge_label_data_source='label',  \n",
    "    graph_height=500,   \n",
    "    links_force_distance=120,     \n",
    "    many_body_force_strength=-50,    \n",
    "    edge_label_size_factor=1.2,\n",
    ")\n",
    "\n",
    "# Display the interactive graph\n",
    "graph.display(inline=True)"
   ]
  }
 ]
}