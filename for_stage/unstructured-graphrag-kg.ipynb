{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f17c12-123f-4048-90c5-b2cba013cfd5",
   "metadata": {},
   "source": [
    "# Tokenflow: Running the RAI GRS Pipeline and Performing Data Analysis on Agents Data\n",
    "\n",
    "### Pipeline Execution Methods\n",
    "After installing the native app in your Snowflake account, you have three options to run the pipeline for input documents:\n",
    "\n",
    "1. **SQL Worksheet** - Execute pipeline steps directly from a new SQL worksheet in Snowsight TODO: add here link to the worksheet code (config based or the other one)\n",
    "2. **Streamlit User Interface** - Run the pipeline through the application's UI TODO: add here link to the presentation and / or video\n",
    "3. **Python SDK** - Use the Python SDK (currently under development)\n",
    "\n",
    "#### Project Configuration\n",
    "Every new project requires configuration for the customization of the pipeline's execution. This configuration is stored in the `CONFIG` column under the `PROJECTS` table in YAML format and includes prompts for the tasks that require LLM calls, algorithm parameters (e.g., retrieval settings) and other pipeline execution settings.\n",
    "\n",
    "For the purposes of this demo, we have already adapted the prompts for our specific use case (Knowledge Graph -KG- extraction for the Tokenflow agents data), but you can customize the prompts and the whole configuration by opening the YAML file in any text editor, editing the desired parameters, and saving the changes.\n",
    "\n",
    "#### Notebook-Based Pipeline Execution\n",
    "This notebook demonstrates running the pipeline by wrapping SQL statements into Python Snowflake Connector calls. The notebook involves:\n",
    "- Configuration Loading: Read the configuration YAML file from disk\n",
    "- Pipeline Invocation: Execute corresponding pipeline steps using direct SQL calls throught the Python Snowflake Connector\n",
    "- Post-Processing and Analysis: After pipeline execution, the workflow includes:\n",
    "    - Export of the GenAI generated structured data: Export agent data, including agent properties, to a separate Snowflake table\n",
    "    - Visualization: Generate and display the Knowledge Graph visualization\n",
    "\n",
    "\n",
    "**Note:** In this notebook, we demonstrate only the basic pipeline execution. However, the application also supports fine-tuning one of the available Cortex LLMs on your own documents. For the purposes of this demo, we use the default LLMs for both the extraction and question answering steps.\n",
    "\n",
    "**Alternative Workflow:** If you have already run the pipeline through the UI, you can use this notebook by skipping the initial sections related to pipeline execution. Instead, go directly to the *'Load extracted graph data from Snowflake'* section. This allows you to proceed straight to the post-processing and visualization phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759050f-aafd-4605-993f-256563453142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timezone\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961df133-10bb-4cc2-9ff2-78421e4a9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"snowflake-connector-python[pandas]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5854d-522c-4507-8b80-8a4e9940e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Snowflake.\n",
    "# Note: MFA must be temporarily disabled on the Snowflake account before running code with the Snowpark Python connector.\n",
    "# You can do this by running the following query in a new SQL Worksheet inside Snowsight: ALTER USER <your_username> SET MINS_TO_BYPASS_MFA = 900;\n",
    "\n",
    "# You can find the account_identifier by running the following query in Snowflake: SELECT CURRENT_ORGANIZATION_NAME() || '-' || CURRENT_ACCOUNT_NAME();\n",
    "account_identifier = \"NDSOEBE-RAI_PROD_GEN_AI_AWS_US_WEST_2_CONSUMER\"  \n",
    "# Your credentials for logging into the same Snowflake account.\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "# Use consumer role for the usage of the app.\n",
    "role = \"RAI_GRS_CONSUMER_ADMIN_ROLE\"\n",
    "# The database name is the native app's installation name.\n",
    "database = \"RAI_GRS_ILIAS\"\n",
    "# The APP schema contains the functions and procedures, while the DATA schema holds the related data from pipeline execution.\n",
    "schema = \"DATA\"  \n",
    "# Your warehouse name.\n",
    "warehouse = \"RAI_GRS_WAREHOUSE\"\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "                user=username,\n",
    "                password=password,\n",
    "                account=account_identifier,\n",
    "                role=role, \n",
    "                database=database,\n",
    "                schema=schema,\n",
    "                warehouse=warehouse\n",
    "            )\n",
    "            \n",
    "            \n",
    "# Create a cursor for this connection.\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4b882-8666-4375-b778-b109ca810cdb",
   "metadata": {},
   "source": [
    "## Pipeline's configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9834617-6219-4a92-af74-d93920d42242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the configuration for this experiment running.\n",
    "with open('config.yml', 'r') as file:\n",
    "    configuration_yaml = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee65a5-06d9-4cdd-b783-a9c83a71f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the config: if you want to make any changes, just edit the file using a text editor, save your changes and rerun the above cell.\n",
    "# configuration_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737ca4c-1859-471b-aa6e-4c1835dba27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "todo: search for TODO in all notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f6ed4-2a39-49cc-abc1-518da9710d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The configuration has been loaded as a Python dictionary, so we can access its individual parts directly.\n",
    "project_id = configuration_yaml['config']['project']['id']\n",
    "project_name = configuration_yaml['config']['project']['name']\n",
    "project_comments = configuration_yaml['config']['project']['comments']\n",
    "\n",
    "print(f'Project with id \"{project_id}\" has comments \"{project_comments}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ce6eb-b7e1-40f5-8328-851a0f4b6bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_suffix = \"\"  # Empty if you want to run it on RAI_GRS\n",
    "app_suffix = \"_ILIAS\"# TODO: delete this and replace \"{app_suffix}\" with empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6ea84-fc90-43b7-8ebf-15132583dc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If this is a new project, we need to insert it as a new record into the PROJECTS table.\n",
    "# All changes made using the cursor are automatically saved to the SF cloud database.\n",
    "# Important: Run this only once!\n",
    "\n",
    "\n",
    "extraction_call = f\"\"\"\n",
    "    INSERT INTO RAI_GRS{app_suffix}.DATA.PROJECTS (ID, NAME, CONFIG, COMMENTS)\n",
    "    VALUES (%s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the SQL call.\n",
    "cursor.execute(extraction_call, (\n",
    "    project_id,\n",
    "    project_name,\n",
    "    None, # We will upload later the configuration.\n",
    "    project_comments\n",
    "))\n",
    "    \n",
    "\n",
    "# Fetch and print all rows returned by the Snowflake query executed via the cursor\n",
    "results = cursor.fetchall() \n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e21ef5-1f3d-4033-be3e-ee407b73e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will upload the configuration.\n",
    "# Prepare config for uploading.\n",
    "json_str = json.dumps(configuration_yaml)\n",
    "\n",
    "# Use binding ‚Äì the connector will do the proper quoting/escaping\n",
    "sql = f\"\"\"\n",
    "CALL RAI_GRS{app_suffix}.app.save_config(\n",
    "   %s,\n",
    "   PARSE_JSON(%s)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(sql, (project_id, json_str))\n",
    "\n",
    "results = cursor.fetchall() \n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e52ea7-f798-4fcc-acf9-20a3810b8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of parameters from the config.\n",
    "# Now that we have saved the configuration to Snowflake there is no need to define the parameters on the calls but we do \n",
    "# it for showcasing the specific parameters of the services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8579a-5051-4d2b-bc00-3182c440b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configuration_yaml['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2090c-44ff-4e63-a5d9-76d9712bd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = config['auth']['openai_api_key']\n",
    "llm_family = config['models']['family']\n",
    "completion_model = config['models']['completion']\n",
    "is_fine_tuned_completion_model = config['models']['is_fine_tuned']\n",
    "embeddings_model = config['models']['embeddings']\n",
    "summarization_context = config['operations']['get_embeddings']['summarization_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10701775-8d4d-4f75-b66e-40b78fe8dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_top_k = config['operations']['question_answering']['retrieval']['similarity_top_k']\n",
    "similarity_threshold = config['operations']['question_answering']['retrieval']['similarity_threshold']\n",
    "retriever_type = config['operations']['question_answering']['retrieval']['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f974bf-09c2-4edd-8500-81b887ccbf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(openai_api_key)\n",
    "print(llm_family)\n",
    "print(completion_model)\n",
    "print(is_fine_tuned_completion_model)\n",
    "print(embeddings_model)\n",
    "print(summarization_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e20175-9125-493a-b3ea-5d25ca6886d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similarity_top_k)\n",
    "print(similarity_threshold)\n",
    "print(retriever_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe29d05-7985-4c9a-b4ba-42ef8c781b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: ADD THE OPENAI API KEY AND REMOVE IT FROM THE FILE BEFORE COMMIT alSO REMOVE YOUR ACCOUNT NAME AND CREDENTIALS\n",
    "# TODO: USE THE ACC NAME OF THE SNOWFLAKE THAT ALEX HAS SENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9936d-b5b7-4bc3-9c06-03796ed4b779",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "\n",
    "Here, we outline each step of the pipeline one by one. Since the configuration file has already been defined and saved in Snowflake, specifying input parameters for each call is not necessary. We only need to provide the `project_id`; the service will then retrieve the required parameters for each step (for example, `similarity_top_k` for retrieval) from the configuration stored in the `PROJECTS` table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e306863-6a58-4b80-a282-f3f033bf9bf8",
   "metadata": {},
   "source": [
    "#### 0. Corpus conversion\n",
    "The very first step is to upload your documents to the `FILES` stage (under the schema `RAI_GRS.DATA`) in a new folder named after the `project_id`. Once uploaded, you can run the corpus conversion, which extracts text content from your documents.\n",
    "\n",
    "There are two options available: standard conversion and visual parsing. Visual parsing leverages an LLM with vision capabilities to interpret and extract meaning from visual content in your documents, such as images and diagrams.\n",
    "\n",
    "For this example, we already have a table with textual data, so there are no PDF documents to process through the corpus conversion endpoint. Instead, we take the relevant textual column from the input file, and format it to match the structure of the `CORPUS` table, as it would appear after running the conversion on PDF documents (i.e., with the same columns and structure). \n",
    "\n",
    "Once formatted, we upload this data to Snowflake so it is ready for the first step of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d79ed-ce37-4961-b4f9-62c1ef039b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 0\n",
    "# # -- Corpus conversion (e.g. PDF to MarkDown).\n",
    "# # No needed here as we already have the raw text in a CSV file, so we will provide the corpus table.\n",
    "# # But if we had uploaded some PDF documents in a folder, then we sould run the following statement:\n",
    "\n",
    "# # Form the SQL call.\n",
    "# extraction_call = f\"\"\"CALL RAI_GRS{app_suffix}.app.execute_convert_corpus('{project_id}');\"\"\"\n",
    "\n",
    "# # Or if the documents contain visual content, we can run this process (note that it may be costly due to extensive LLM calls): \n",
    "# # extraction_call = f\"\"\"CALL RAI_GRS{app_suffix}.app.execute_llm_convert_corpus('{project_id}');\"\"\"\n",
    "\n",
    "# # Execute the SQL call.\n",
    "# cursor.execute(extraction_call)\n",
    "\n",
    "# # See the results of the call.\n",
    "# results = cursor.fetchall() \n",
    "# for row in results:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca275d45-27d0-42e3-8cde-c29eae9a9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the table of interest that has the textual data. This is the csv with the raw input data.\n",
    "file_path = \"../../data/virtuals-agents.csv\"\n",
    "virtuals_agents_raw = pd.read_csv(file_path)\n",
    "virtuals_agents_raw.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940f07d-3c0f-42dd-b2d5-707fb9d1acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this experiment, we extract the name, symbol, and description of the agents, and combine them into a single string.\n",
    "# This aggregated text will serve as the input document (one for each agent) for our algorithm.\n",
    "corpus = virtuals_agents_raw[[\"NAME\", \"SYMBOL\", \"DESCRIPTION\"]].copy()\n",
    "\n",
    "# Create the necessary columns to match the schema of the Snowflake corpus table.\n",
    "corpus[\"PROJECT_ID\"] = project_id\n",
    "corpus[\"CHUNK_ID\"] = corpus[\"NAME\"].apply(lambda x: f\"{project_id}/{x}\")\n",
    "\n",
    "# We could have custom metadata for the different documents, or let the LLM generate some (e.g. short title) but for now we use the same \n",
    "# metadata for all the records.\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "formatted_time = now_utc.strftime(\"D:%Y%m%d%H%M%SZ\")\n",
    "metadata_for_all_entries = {\n",
    "  \"creationDate\": formatted_time,\n",
    "  \"subject\": \"Description of Tokenflow agents.\",\n",
    "  \"source\": file_path\n",
    "}\n",
    "corpus[\"METADATA\"] = corpus.apply(lambda _: metadata_for_all_entries, axis=1)\n",
    "corpus[\"CONTENT\"] = corpus.apply(lambda row: f\"Agent with name {row['NAME']} has symbol: {row['SYMBOL']} and description: {row['DESCRIPTION']}.\", axis=1)\n",
    "\n",
    "# The final table format is matches the CORPUS table schema on Snowflake.\n",
    "final_corpus_df = corpus[[\"PROJECT_ID\", \"CHUNK_ID\", \"CONTENT\", \"METADATA\"]]\n",
    "final_corpus_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1723912-1822-4eb9-ad53-76b5c708b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have ready the textual data into the corpus table, we upload it to SF instead of running the corpus conversion step.\n",
    "# Then we can run the GraphRAG Native App pipeline.\n",
    "\n",
    "# Upload corpus table to snowflake usint write_pandas from Snowflake API.\n",
    "success, nchunks, nrows, _ = write_pandas(conn=conn,\n",
    "                                          df=final_corpus_df,\n",
    "                                          database=f'RAI_GRS{app_suffix}',\n",
    "                                          schema='DATA',\n",
    "                                          table_name='CORPUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36de125-0001-4995-b748-cc14911e7b0a",
   "metadata": {},
   "source": [
    "#### 1. Entities and relations extraction (with customization of the relative prompt)\n",
    "\n",
    "After preparing the text documents, we can begin our pipeline with the first actual step of the knowledge graph (KG) construction. \n",
    "\n",
    "In this example, we demonstrate how to customize the prompt to better fit specific needs. But how can we customize the prompt, and what does that mean for our task?\n",
    "\n",
    "Let‚Äôs explore this through the following example:\n",
    "\n",
    "Suppose, based on prior analysis of our agent description data‚Äîor from specific business requirements‚Äîwe want to extract the following five properties for each agent:\n",
    "\n",
    "1. Purpose/Function  \n",
    "2. Character and Personality  \n",
    "3. Collaborations with other agents  \n",
    "4. Skills/Abilities  \n",
    "5. Key Elements / Expertise / Specialty / Target\n",
    "\n",
    "We recognize that some agents may have incomplete descriptions, so these values may not be available for every entry. Nevertheless, we can instruct the LLM to extract these specific fields as node properties during the KG extraction process.\n",
    "\n",
    "We‚Äôve already configured this behavior by modifying the prompt in the `config.yml` file, under the `get_entities_relations` section. The prompt also includes context about the domain of the documents to improve extraction accuracy. \n",
    "\n",
    "If you don't have specific customization requirements, you can keep the default prompt that comes with the installation of the app‚Äîit has been written to perform well across a variety of document types and domains.\n",
    "\n",
    "Once the prompt is set and the desired LLM for extraction is selected, we're ready to run the extraction step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df9004-7bb5-48c7-9e84-d3895c92b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# -- Entities / relations extraction.\n",
    "\n",
    "# Form the SQL call.\n",
    "extraction_call = f\"\"\"CALL RAI_GRS{app_suffix}.app.execute_get_entities_relations('{project_id}');\"\"\"\n",
    "\n",
    "# Execute the SQL call.\n",
    "cursor.execute(extraction_call)\n",
    "\n",
    "# See the results of the call.\n",
    "results = cursor.fetchall() \n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd38ad3-9a51-4233-bead-5cc0ab4dc493",
   "metadata": {},
   "source": [
    "#### 2. Community detection\n",
    "\n",
    "After extracting entities and relations, the next step is community detection. Several algorithms are available for this task, with configurable parameters. For example, you can set a maximum community size to prevent the formation of overly large communities with too many nodes. With the configuration saved, we can proceed directly to executing the `execute_get_communities` procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826505a-595e-4a00-bd81-b04030439b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# -- Community detection.\n",
    "\n",
    "# Form the SQL call.\n",
    "extraction_call = f\"\"\"CALL RAI_GRS{app_suffix}.app.execute_get_communities('{project_id}');\"\"\"\n",
    "\n",
    "# Execute the SQL call.\n",
    "cursor.execute(extraction_call)\n",
    "\n",
    "# See the results of the call.\n",
    "results = cursor.fetchall() \n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2097132-8fb4-453e-9de3-cf9d103e3b61",
   "metadata": {},
   "source": [
    "#### 3. Graph indexing: summarization and embeddings\n",
    "\n",
    "This step performs the following operations:\n",
    "\n",
    "- Summarization of each community with LLM, capturing the context of the nodes it contains  \n",
    "- Embedding generation for the `CORPUS` table, node and edge properties, and the community summaries\n",
    "\n",
    "As with any LLM task of your pipeline, you can adjust the summarization prompt to guide the LLM on the desired level of abstraction and which details to include in the summaries. In this case, we use the default prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159817f-9564-4d32-94ce-5c410dbfbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# -- Graph indexing: summarization and embeddings.\n",
    "\n",
    "# Form the SQL call.\n",
    "extraction_call = f\"\"\"CALL RAI_GRS{app_suffix}.app.execute_get_embeddings('{project_id}');\"\"\"\n",
    "\n",
    "# Execute the SQL call.\n",
    "cursor.execute(extraction_call)\n",
    "\n",
    "# See the results of the call.\n",
    "results = cursor.fetchall() \n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23340232-d6a8-408f-ac33-899932443209",
   "metadata": {},
   "source": [
    "#### 4. Question Answering (QA)\n",
    "\n",
    "After indexing completes, we are ready to use our app for question answering!\n",
    "\n",
    "Note that the indexing phase occurs not only during embedding generation with your selected embedder but also within the Cortex Search service.\n",
    "\n",
    "This provides two retrieval options:\n",
    "- **Vector search**, which uses the embeddings generated in the previous step (step #3)  \n",
    "- **Cortex Search**, the managed service provided by Snowflake Cortex, which performs hybrid retrieval and reranking behind the scenes\n",
    "\n",
    "As shown in the `config.yml`, we have selected Cortex Search for this demo and set `similarity_top_k=10`. \n",
    "\n",
    "Whenever you need to change retrieval settings, simply edit the YAML file and rerun the cell containing the `save_config` call.\n",
    "\n",
    "You can also select which sources to include in the retrieved results, since there are multiple content types to search: corpus items, community summaries, and verbalized properties of both nodes and edges. For now, we use the default behavior, which searches the most relevant across all available sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171fd66-7108-47ed-860c-c4a541b3959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Selected retriever for this demo uses \"{retriever_type}\" with \"similarity_top_k\" set to {similarity_top_k} and \"similarity_threshold\" to {similarity_threshold}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984be40-0084-4f04-95e1-9fb41df020bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "# Form the SQL call.\n",
    "# question = \"What is the meaning of the context?\"\n",
    "question = \"What do you know about WAI Combinator?\"\n",
    "extraction_call = f\"\"\"CALL RAI_GRS{app_suffix}.app.execute_get_answer('{project_id}', '{question}');\"\"\"\n",
    "\n",
    "# Execute the SQL call.\n",
    "cursor.execute(extraction_call)\n",
    "\n",
    "# See the results of the call.\n",
    "result = cursor.fetchone() \n",
    "answer = result[1]\n",
    "context = json.loads(result[2])\n",
    "print(f\"Question: {question}\")\n",
    "print()\n",
    "print(f\"Answer: {answer}\")\n",
    "print()\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"Context has {len(context)} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03626b8-ffd5-493c-9c39-38637129004a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Load extracted graph data from snowflake \n",
    "\n",
    "The native app proci\n",
    "After running our pipeline we can download the graph data to use them for other\n",
    "\n",
    "We use the provided method from Snowflake Python connector to download the graph data:\n",
    "https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#fetch_pandas_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaabd4f-0416-4439-b2f0-6419b6d66efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.execute(f\"SELECT * FROM RAI_GRS{app_suffix}.DATA.CORPUS WHERE PROJECT_ID='{project_id}'\")\n",
    "# # Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n",
    "# corpus = cursor.fetch_pandas_all()\n",
    "# corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c55c9-9e44-40c4-8e84-2b61170c5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT * FROM RAI_GRS{app_suffix}.DATA.NODES WHERE PROJECT_ID='{project_id}'\")\n",
    "# Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n",
    "nodes = cursor.fetch_pandas_all()\n",
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5654147-4196-4858-a505-4a46659fb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT * FROM RAI_GRS{app_suffix}.DATA.NODE_PROPERTIES WHERE PROJECT_ID='{project_id}'\")\n",
    "node_properties = cursor.fetch_pandas_all()\n",
    "node_properties.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a42ab-9830-48de-a4d7-739bdaec6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT * FROM RAI_GRS{app_suffix}.DATA.EDGES WHERE PROJECT_ID='{project_id}'\")\n",
    "edges = cursor.fetch_pandas_all()\n",
    "edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ace105-bcb3-4403-bf3e-bbb60c7e80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT * FROM RAI_GRS{app_suffix}.DATA.EDGE_PROPERTIES WHERE PROJECT_ID='{project_id}'\")\n",
    "edge_properties = cursor.fetch_pandas_all()\n",
    "edge_properties.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a06cfc-d625-4b0d-ba1a-0e440ec233eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT * FROM RAI_GRS{app_suffix}.DATA.COMMUNITIES WHERE PROJECT_ID='{project_id}'\")\n",
    "communities = cursor.fetch_pandas_all()\n",
    "communities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5e41e-3362-423c-bc47-1a0abf76b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aeea18-0108-4f62-81b9-003388f43d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case we want to save our graph data locally and make them available for next time without needing to connect to SF \n",
    "# again and use the cursor to download the data.\n",
    "# nodes.to_csv(r\"data/output/output from native app run/nodes.csv\", index=False)\n",
    "# node_properties.to_csv(r\"data/output/output from native app run/node_properties.csv\", index=False)\n",
    "# edges.to_csv(r\"data/output/output from native app run/edges.csv\", index=False)\n",
    "# communities.to_csv(r\"data/output/output from native app run/communities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d940781-1865-4d6f-a6d9-31b230266229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If we wanted to use pandas read_csv after downloading the tables as csv outputs.\n",
    "\n",
    "# nodes = pd.read_csv(r\"data/output/output from native app run/nodes.csv\")\n",
    "# node_properties = pd.read_csv(\"data/output/output from native app run/node_properties.csv\")\n",
    "# edges = pd.read_csv(\"data/output/output from native app run/edges.csv\")\n",
    "# communities = pd.read_csv(\"data/output/output from native app run/communities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db140cd2-f68e-430f-91c7-9ef35ef9ae4a",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0daf13-7cd9-47a4-9291-1031125ceff6",
   "metadata": {},
   "source": [
    "### Extracted nodes overview\n",
    "Let's see the nodes and how many of them are agents (they have \"ai_agent\" as type.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b710789-fa88-47ba-a7b0-4db3672f3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e91a45-d0d5-4d3a-ad4a-730fb6911fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['TYPE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56417ff8-f48b-4bcd-a185-d2c1458bd5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many agents have been extracted?\n",
    "nodes[nodes['TYPE']=='ai_agent'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc09cad-b300-4703-8544-ba18e97de7fe",
   "metadata": {},
   "source": [
    "There are more than 100 agents while the corpus is of 100 descriptions. So, let us see the extra agent nodes: should we have one agent node for each corpus item? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524641d-0a08-4b7d-8237-95389acbc348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase to count duplicated agents that might appear\n",
    "# nodes['ID_lowercase'] = nodes['ID'].str.lower()\n",
    "only_agent_nodes = nodes[nodes['TYPE']=='ai_agent']\n",
    "# only_agent_nodes.loc[:, 'ID_lowercase'] = only_agent_nodes['ID'].str.lower()\n",
    "print(f\"Duplicated count: {only_agent_nodes.duplicated(subset=['ID']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48292ae-83fd-4c0e-8cf8-3f08bd939475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove agents with the same name.\n",
    "condition = (nodes.duplicated(subset='ID')) & (nodes['TYPE'] == 'ai_agent')\n",
    "nodes = nodes[~condition]\n",
    "nodes[nodes['TYPE']=='ai_agent'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f55202-b40e-4070-a6f2-ee09d3b01985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the agents whose names differ from the original filenames, in order to see if some descriptions contain more\n",
    "# than one agents.\n",
    "only_agent_nodes.loc[:, 'file_name'] = only_agent_nodes['CHUNK_ID'].str.replace(f\"{project_id}/\", \"\") \n",
    "only_agent_nodes[only_agent_nodes['ID'] != only_agent_nodes['file_name']][['CHUNK_ID', 'ID', 'CONTEXT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b286f8-aa2a-4595-ae39-038a685bd6a2",
   "metadata": {},
   "source": [
    "As we can see, some descriptions mention more than one agent, so it is appropriate to extract multiple agents from the same text.\n",
    "\n",
    "Examples where the text mentions more than one agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68c6a9-7945-48e6-b9ca-db992f7b97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_agent_nodes[only_agent_nodes['CHUNK_ID']=='tokenflow/Zenith'].CONTEXT.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633291b-f184-4432-89d0-65a1b3fccf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_agent_nodes[only_agent_nodes['CHUNK_ID']=='tokenflow/DXAI.app'].CONTEXT.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c4f01-29b7-4882-85d5-061770abfbd2",
   "metadata": {},
   "source": [
    "#### Add the properties and communities to the nodes dataframe for convinience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f1d9a-d897-4f82-8143-5af425a55ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_properties_for_node(node_id, chunk_id, node_properties_df) -> dict:  \n",
    "    \"\"\"\n",
    "    Retrieve all properties for a given node.\n",
    "\n",
    "    This function searches in `node_properties_df` to find all properties \n",
    "    associated with the specified `node_id` and returns them as a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        node_id (str): The unique identifier of the node.\n",
    "        chunk_id (str): The unique identifier of the chunk id from which the node has been extracted.\n",
    "\n",
    "    Returns:\n",
    "        dict: None if no properties found. If there are properties, a dictionary where each key is a \n",
    "              property name and the corresponding value is the property value for the given node.\n",
    "    \"\"\"\n",
    "    # Search in the properties df with the properties from all nodes to find the properties of this node.\n",
    "    properties_of_this_node = node_properties_df[\n",
    "        (node_properties_df['NODE_ID'] == node_id) &\n",
    "        (node_properties_df['CHUNK_ID'] == chunk_id)\n",
    "    ]\n",
    "\n",
    "    if not properties_of_this_node.empty:\n",
    "        # Remove duplicates based on 'PROPERTY_NAME' and 'PROPERTY_VALUE'\n",
    "        unique_properties = properties_of_this_node.drop_duplicates(subset=['PROPERTY_NAME', 'PROPERTY_VALUE'])\n",
    "        # Convert to a dictionary: PROPERTY_NAME -> PROPERTY_VALUE\n",
    "        property_dict = dict(zip(unique_properties['PROPERTY_NAME'], unique_properties['PROPERTY_VALUE']))\n",
    "        return property_dict\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73523001-59de-4959-bab9-d84e310784a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the prompt we asked that the five importand properties will set to 'null' if they are not available. So, here we replace with None.\n",
    "def clean_property_value(val):\n",
    "    if val == []:\n",
    "        return None\n",
    "    if val == '[]':\n",
    "        return None\n",
    "    if val == \"null\":\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "node_properties['PROPERTY_VALUE'] = node_properties['PROPERTY_VALUE'].apply(clean_property_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685a3c3-cef3-40ca-9bbe-9a9d07689364",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3263f-66a9-4602-a574-cb13126fbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e3bc5-16b0-4ac3-9d67-50a0170d1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to store properties as dict in this dataframe.\n",
    "nodes['PROPERTIES'] = nodes.apply(\n",
    "    lambda row: get_properties_for_node(row['ID'], row['CHUNK_ID'], node_properties),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2a358-59d8-480a-bc30-a31258d593a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45ea1e-e6c2-45ed-aba5-652746a9e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will add the communities too.\n",
    "communities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d6dd9-4ffa-41f2-8dce-383cab8c4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a137390-9c02-4f66-876f-d984d7735126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the nodes and communities dataframes\n",
    "nodes = pd.merge(\n",
    "    left=nodes,\n",
    "    right=communities,\n",
    "    how='left',\n",
    "    left_on=['ID'],\n",
    "    right_on=['NODE_ID']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90934b78-4ccb-4017-bf97-22fea92c6b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes = nodes.drop(columns=['PROJECT_ID_y', 'NODE_ID'])\n",
    "nodes = nodes.rename(columns={'PROJECT_ID_x': 'PROJECT_ID'})\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4fa96-cde3-4a88-9fa9-a6f35df97d1b",
   "metadata": {},
   "source": [
    "## Graph byproduct: agents data analysis\n",
    "\n",
    "### Take the agents data and store them in a new Snowflake table\n",
    "Here, we export only the agent nodes along with the five properties we requested in the extraction prompt. We filter the agent nodes and create separate columns for each of these five properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881f3b3-b0f8-400b-9f66-eb3433f37047",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = nodes[nodes['TYPE']=='ai_agent']\n",
    "agents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69023f94-c919-44a8-b361-2e542855351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34b6e0-215b-4955-9b31-871179184502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents = agents.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d34ab2-af59-404a-ac8d-d42ca46a7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9dbb4c-edfc-4d9c-b729-0fec5ea4cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all agents have the five properties as keys.\n",
    "count = 0\n",
    "for property_set in agents['PROPERTIES'].to_list():\n",
    "    if not all(key in property_set for key in ['purpose', 'character', 'collaborators', 'key_elements', 'skills']):\n",
    "        count += 1\n",
    "        print(\"We found an agent that has not all the five main properties. Let's fix that by assing them to the agent with None.\")\n",
    "        for key in ['purpose', 'character', 'collaborators', 'key_elements', 'skills']:\n",
    "            if key not in property_set:\n",
    "                property_set[key] = None\n",
    "                \n",
    "print(f\"There were {count} agents with missing properties.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf09f0-22ca-4610-8cb6-f8508cf0c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the five important properties and place them as separate columns.\n",
    "\n",
    "# Define keys to extract\n",
    "key_properties_to_extract = ['purpose', 'character', 'collaborators', 'key_elements', 'skills']\n",
    "\n",
    "# Function to extract keys\n",
    "def extract_properties(prop_dict):\n",
    "    extracted = {key: prop_dict.get(key) for key in key_properties_to_extract}\n",
    "    other = {k: v for k, v in prop_dict.items() if k not in key_properties_to_extract}\n",
    "    extracted['other properties'] = other\n",
    "    return pd.Series(extracted)\n",
    "\n",
    "# Apply extraction\n",
    "df_extracted = agents['PROPERTIES'].apply(extract_properties)\n",
    "\n",
    "# Combine with original dataframe\n",
    "agents = pd.concat([agents.drop(columns=['PROPERTIES']), df_extracted], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf1ef2-475d-4f45-b726-022b18fcd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns and keep those we are need.\n",
    "agents = agents.rename(columns={'ID': 'Name',\n",
    "                                'CHUNK_ID': 'Original filename', \n",
    "                                'CONTEXT': 'Description',\n",
    "                                'other properties': 'Other properties',\n",
    "                                'skills': 'Skills',\n",
    "                                'key_elements': 'Key elements',\n",
    "                                'purpose': 'Purpose',\n",
    "                                'character': 'Character',\n",
    "                                'collaborators': 'Collaborators'\n",
    "                               })\n",
    "agents = agents.drop(columns=['PROJECT_ID', 'TYPE', 'COMMUNITY_ID'])\n",
    "agents.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a971bd-8451-47af-95f7-22508f204e0f",
   "metadata": {},
   "source": [
    "#### Store the agents in a new Snowflake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36197745-2dad-432b-87ac-e3d4d7b6f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "                user=username,\n",
    "                password=password,\n",
    "                account=account_identifier,\n",
    "                role=role, \n",
    "                database=database,\n",
    "                schema=schema,\n",
    "                warehouse=warehouse\n",
    "            )\n",
    "            \n",
    "            \n",
    "# Create a cursor for this connection (again).\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201319dc-ef14-44a1-9801-a46af9d1d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(f\"\"\"\n",
    "DROP TABLE IF EXISTS RAI_GRS{app_suffix}.DATA.TOKENFLOW_AGENTS;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71b7aa-6fa7-48c6-8706-fe3784e5ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb9528-e4e6-49b6-9071-5e2448de1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE RAI_GRS{app_suffix}.DATA.TOKENFLOW_AGENTS (\n",
    "    \"Original filename\" VARCHAR,\n",
    "    \"Name\" VARCHAR,\n",
    "    \"Description\" VARCHAR,\n",
    "    \"Purpose\" VARCHAR,\n",
    "    \"Character\" VARCHAR,\n",
    "    \"Collaborators\" VARCHAR,\n",
    "    \"Key elements\" VARCHAR,\n",
    "    \"Skills\" VARCHAR,\n",
    "    \"Other properties\" VARCHAR\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c0c72-5b3a-41aa-a225-d20e9106a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to a new SF table.\n",
    "success, nchunks, nrows, _ = write_pandas(conn=conn,\n",
    "                                          df=agents,\n",
    "                                          database=f'RAI_GRS{app_suffix}',\n",
    "                                          schema='DATA',\n",
    "                                          table_name='TOKENFLOW_AGENTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b9795-5331-418c-95fd-a45c4e5e0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents.to_csv(\"data/output/other outputs/agents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ef1c8-da4f-4d86-a170-06ed1ec9cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close this cursor.\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eca687-5553-4dd6-9663-9d3ac588a9f6",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600c743-f457-45fe-a49c-431632357eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35592725-c80f-427b-aeb5-c5c76ea1c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_icon(node_type):\n",
    "    \"\"\"Get appropriate icon based on node type\"\"\"\n",
    "    icon_map = {\n",
    "        'ü§ñ': ['ai_agent', 'ai', 'ai_technology', 'ai_framework'],\n",
    "        'üßë': ['person', 'user', 'family_member'],\n",
    "        'üñ•Ô∏è': ['platform', 'software', 'technology', 'feature'],\n",
    "        '‚Çø': ['blockchain', 'cryptocurrency', 'trading_platform', 'token', 'blockchain_paradise', 'meme_coin'],\n",
    "        'ü¶æ': ['ai_agent_role'],\n",
    "        'üí∞': ['financial_product', 'currency'],\n",
    "        'üß™': ['product'],\n",
    "        'üè¢': ['company', 'organization'],\n",
    "        'üìÑ': ['document'],\n",
    "        'üåç': ['country', 'place', 'ecosystem'],\n",
    "        'üéñÔ∏è': ['certification'],\n",
    "        'üìú': ['regulation', 'legal', 'protocol', 'algorithm'],\n",
    "        'üìå': ['default']  # Default case\n",
    "    }\n",
    "\n",
    "    # Flatten dictionary for quick lookup\n",
    "    node_to_icon = {key: icon for icon, keys in icon_map.items() for key in keys}\n",
    "\n",
    "    return node_to_icon.get(node_type, 'üìå')  # Return default icon if not found\n",
    "\n",
    "# # Example usage\n",
    "# print(get_node_icon('company'))  # üè¢\n",
    "# print(get_node_icon('chemical'))  # üß´\n",
    "# print(get_node_icon('unknown'))  # üìå (default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62657fc-703e-4318-8fb8-d134f9c9e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_color(node_type):\n",
    "    \"\"\"Get appropriate color based on node type\"\"\"\n",
    "    color_map = {\n",
    "        '#FFB6C1': ['ai_agent', 'ai', 'ai_technology', 'ai_framework'],  # Light pink\n",
    "        '#DAA06D': ['person', 'user', 'family_member'], # Brown\n",
    "        '#98FB98': ['platform', 'software', 'technology', 'feature'],  # Pale green\n",
    "        '#4682B4': ['blockchain', 'cryptocurrency', 'trading_platform', 'token', 'blockchain_paradise', 'meme_coin'],  # Steel blue\n",
    "        '#FFD700': ['ai_agent_role'],  # Gold\n",
    "        '#FFA500': ['financial_product', 'currency'],  # Orange\n",
    "        '#90EE90': ['company', 'organization'],  # Light green\n",
    "        '#ADD8E6': ['document'],  # Light blue\n",
    "        '#DDA0DD': ['country', 'place'],  # Plum\n",
    "        '#DC143C': ['certification'],  # Crimson\n",
    "        '#8FBC8F': ['regulation', 'legal', 'protocol', 'algorithm'],  # Dark sea green\n",
    "        '#F0F0F0': ['default']  # Light gray\n",
    "    }\n",
    "\n",
    "    # Flatten dictionary for quick lookup\n",
    "    node_to_color = {key: color for color, keys in color_map.items() for key in keys}\n",
    "\n",
    "    return node_to_color.get(node_type, '#F0F0F0')  # Return default color if not found\n",
    "\n",
    "# # Example usage\n",
    "# print(get_node_color('company'))  # #90EE90 (Light green)\n",
    "# print(get_node_color('chemical'))  # #FFC0CB (Pink)\n",
    "# print(get_node_color('unknown'))  # #F0F0F0 (Default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13558549-3d85-4d6c-9bba-3b4a0ab42338",
   "metadata": {},
   "source": [
    "### Using the yFiles library\n",
    "The library `yfiles_jupyter_graphs` is not supported for direct usage on Snowflake [notebooks](https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks-import-packages) (you must upload the library on a stage and try to use it from there), so we use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be6c3e-2a3b-44b5-81d0-48901bc3766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7c69c-9d02-40be-9790-c48d957ac23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yfiles_jupyter_graphs import GraphWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd3b3f-dccb-4244-b56c-6e699e313b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb9575-a3b4-4a95-95d0-8563fe24cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Store the postprocessed data, so you can read them later for visualization. \n",
    "# # But keep in mind that in that case you will need to pay attention to converting the 'PROPERTIES' column of nodes back to list of dicts\n",
    "# # using the safe_eval function in the next cell.\n",
    "# nodes.to_csv('data/output from postprocessing on notebook/nodes_with_properties.csv', index=False)\n",
    "# edges.to_csv('data/output from postprocessing on notebook/edges.csv', index=False)  # Here this df is the same we the input\n",
    "# agents.to_csv('data/output from postprocessing on notebook/agents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc72149-6974-422a-a8b1-eceb7dcf9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely parse the 'PROPERTIES' column, ignoring NaN\n",
    "def safe_eval(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {val}\\n{e}\")\n",
    "        return val  # Or return np.nan if you prefer to drop bad values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfff620-bf08-4be5-bbd6-757c8da9235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = pd.read_csv('data/nodes_with_properties.csv')\n",
    "# nodes['PROPERTIES'] = nodes['PROPERTIES'].apply(safe_eval)\n",
    "\n",
    "# edges = pd.read_csv('data/edges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d985b46-b895-49be-ab3e-dfec30f0bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044d790-99e7-4f27-8534-c6cba5b9a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08f71d-9bcf-4af5-8690-020b32d7df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_for_yfiles = []\n",
    "\n",
    "for index, row in nodes.iterrows(): \n",
    "    # Check if the node already exists in the graph based on node ID.\n",
    "    if any(node['id'] == row['ID'] for node in nodes_for_yfiles):\n",
    "        continue  # Skip adding this node if it already exists in the graph.\n",
    "    entity_emoji = get_node_icon(node_type=row['TYPE'])\n",
    "    entity_color = get_node_color(node_type=row['TYPE'])\n",
    "    entity_label = f\"{entity_emoji} {row['ID']}\"\n",
    "    entity_properties = row['PROPERTIES']\n",
    "    if entity_properties is None:\n",
    "        entity_properties = {}\n",
    "        \n",
    "     # Add the node type as metadata in the first position.\n",
    "    entity_properties[\"node_type\"] = row['TYPE']\n",
    "    entity_properties[\"node_id\"] = row['ID']\n",
    "\n",
    "    node_for_yfiles = {\"id\": row['ID'],\n",
    "                       \"properties\":\n",
    "                          {\"label\": entity_label,\n",
    "                           \"properties\": entity_properties,\n",
    "                           \"color\": entity_color,\n",
    "                           \"type\": row['TYPE'],\n",
    "                           \"community\": row['COMMUNITY_ID']\n",
    "                          }\n",
    "                     }\n",
    "    # Add the node.\n",
    "    nodes_for_yfiles.append(node_for_yfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2aeda8-6ba3-4b2b-aaed-d8495135ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_for_yfiles = []\n",
    "\n",
    "for index, row in edges.iterrows():        \n",
    "    edge_for_yfiles = {\n",
    "        \"id\": index,\n",
    "        \"start\": row['SRC_NODE_ID'],\n",
    "        \"end\": row['DST_NODE_ID'],\n",
    "        \"properties\":\n",
    "            {\n",
    "             \"label\": row['TYPE'],\n",
    "            }\n",
    "       }\n",
    "    edges_for_yfiles.append(edge_for_yfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c4fbc-28c2-433e-b396-91dff7f39f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = GraphWidget()\n",
    "w.nodes = nodes_for_yfiles\n",
    "w.edges = edges_for_yfiles\n",
    "w.directed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a62e77-3251-4399-b953-faa4df55e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show with color mapping\n",
    "\n",
    "w.node_color_mapping = 'color'\n",
    "w.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f56565-1009-43e8-b80b-e6890a460245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show with color and community mapping\n",
    "\n",
    "w.node_color_mapping = 'color'\n",
    "w.node_parent_group_mapping = 'community'\n",
    "w.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e460d6-218e-4ac5-8042-d98c9e138ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some nodes seem to not having any edge.\n",
    "# A check:\n",
    "check_node = \"Yugo\"\n",
    "print(check_node in edges['SRC_NODE_ID'].tolist())\n",
    "print(check_node in edges['DST_NODE_ID'].tolist())\n",
    "print()\n",
    "# full_context = nodes[nodes['ID'] == check_node]['CONTEXT'].values[0]\n",
    "# print(full_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
