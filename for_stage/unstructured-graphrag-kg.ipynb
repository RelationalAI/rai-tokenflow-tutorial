{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f17c12-123f-4048-90c5-b2cba013cfd5",
   "metadata": {},
   "source": [
    "# Tokenflow: Running the RAI GRS Pipeline on Agents Data\n",
    "\n",
    "### Pipeline Execution Methods\n",
    "After installing the native app in your Snowflake account, you have three options to run the pipeline for your input documents:\n",
    "\n",
    "1. **SQL Worksheet** ‚Äì Execute pipeline steps directly from a new SQL worksheet in Snowsight. Copy the code from the file `unstructured-graphrag-SQL-workflow.sql` and paste it into a new SQL worksheet on Snowsight.\n",
    "2. **Streamlit User Interface** - Run the pipeline through the application's UI by clicking on the **RAI GRS** app that you can find under the menu `Data Products` ‚Üí `Apps` ([here](https://drive.google.com/drive/u/1/folders/1RCCpP5XSGh8LOi4x-MzpnOmfMBwQ-V8R) you can find some slides for the UI usage).\n",
    "3. **Notebook-Based Pipeline Execution** - You can use this notebook, which leverages the Python Snowflake Connector, to execute the SQL queries one by one.\n",
    "\n",
    "\n",
    "#### Notebook-Based Pipeline Execution\n",
    "This notebook demonstrates running the pipeline by wrapping SQL statements into Python Snowflake Connector calls. The notebook involves:\n",
    "- Configuration Loading: Read the configuration YAML file from disk.\n",
    "- Pipeline Invocation: Execute corresponding pipeline steps using direct SQL calls throught the Python Snowflake Connector.\n",
    "- Visualization: Generate and display the Knowledge Graph visualization.\n",
    "\n",
    "\n",
    "**Note:** In this notebook, we demonstrate only the basic pipeline execution. However, the application also supports fine-tuning one of the available Cortex LLMs on your own documents. For the purposes of this demo, we use the default LLMs for both the extraction and question answering steps.\n",
    "\n",
    "**Alternative Workflow:** If you have already run the pipeline through the UI, you can use this notebook by skipping the initial sections related to pipeline execution. Instead, go directly to the *'Load extracted graph data from Snowflake'* section. This allows you to proceed straight to the visualization phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759050f-aafd-4605-993f-256563453142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import gravis as gv\n",
    "import networkx as nx\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f3ce4-addb-4ea6-bcb2-aa396cf39525",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "conn = snowflake.connector.connect(session=session)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8839037-5085-46a1-afcb-b7b6758f66fd",
   "metadata": {},
   "source": [
    "## Create your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f6ed4-2a39-49cc-abc1-518da9710d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = input(\"Please give your project_id: \")  # e.g. tokenflow\n",
    "project_name = project_id\n",
    "project_comments = f'Project deployment for {project_id}'  # You could add custom comments\n",
    "\n",
    "print(f'Project with id \"{project_id}\" has comments \"{project_comments}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1077f-5e82-4a69-a786-ab7ccbbe8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is a new project, we need to insert it as a new record into the PROJECTS table.\n",
    "# All changes made using the cursor are automatically saved to the SF cloud database.\n",
    "# Define the SQL query with the NOT EXISTS check.\n",
    "extraction_call = f\"\"\"\n",
    "    INSERT INTO RAI_GRS.DATA.PROJECTS (ID, NAME, CONFIG, COMMENTS)\n",
    "    SELECT %s, %s, %s, %s\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM RAI_GRS.DATA.PROJECTS \n",
    "        WHERE ID = %s\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL call, providing the values for the placeholders.\n",
    "cursor.execute(extraction_call, (\n",
    "    project_id,\n",
    "    project_name,\n",
    "    None, # We will use the default config and provide parameters in some of the functions.\n",
    "    project_comments,\n",
    "    project_id  # The ID is also used in the WHERE clause to check for existence.\n",
    "))\n",
    "\n",
    "# Fetch and print all rows returned by the Snowflake query executed via the cursor\n",
    "results = cursor.fetchall() \n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c9ab1-0101-4540-bc3d-c3d1a793435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 'CORPUS' if your run the pipeline for the Tokenflow use case, or 'PDF' if you have uploaded files to the FILES stage.\n",
    "data_source = input(\"Are the documents coming from PDF files in the stage, or will you upload them directly into the corpus table? Type 'pdf' or 'corpus': \")\n",
    "data_source = data_source.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37d9ec-1c6c-4be2-88f5-7fd02945eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_family = input(\"Do you want to use an LLM inside Snowflake or a model from the OpenAI API? Type 'openai' or 'snowflake': \")\n",
    "llm_family = llm_family.lower()\n",
    "if llm_family == 'openai':\n",
    "    openai_api_key = input(\"Please input OpenAI API key: \")\n",
    "else:\n",
    "    openai_api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7399b-d096-4dd9-818c-3a8bdccf3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for OpenAI: \"gpt-4.1\" and \"gpt-4o\".\n",
    "# Examples for Snowflake Cortex: \"claude-3-5-sonnet\" and \"llama3.3-70b\".\n",
    "completion_model = input(\"Select the LLM: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b6c3ab-d98a-4d57-8d2c-6ca2fa057f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI: \"text-embedding-3-small\" and \"text-embedding-3-large\".\n",
    "# Examples for Snowflake Cortex: \"snowflake-arctic-embed-l-v2.0\" and \"e5-base-v2\".\n",
    "embeddings_model = input(\"Select the embeddings: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2090c-44ff-4e63-a5d9-76d9712bd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we provide default values for additional parameters, which can be customized based on the experiment.\n",
    "is_fine_tuned_completion_model = False\n",
    "summarization_context=\"chunk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9936d-b5b7-4bc3-9c06-03796ed4b779",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e306863-6a58-4b80-a282-f3f033bf9bf8",
   "metadata": {},
   "source": [
    "#### 0. Corpus conversion\n",
    "The very first step is to upload your documents to the `FILES` stage (under the schema `RAI_GRS.DATA`) in a new folder named after the `project_id`. Once uploaded, you can run the corpus conversion, which extracts text content from your documents.\n",
    "\n",
    "There are two options available: standard conversion and visual parsing. Visual parsing leverages an LLM with vision capabilities to interpret and extract meaning from visual content in your documents, such as images and diagrams.\n",
    "\n",
    "For this example, we already have a table with textual data, so there are no PDF documents to process through the corpus conversion endpoint. Instead, we take the relevant textual column from the input file, and format it to match the structure of the `CORPUS` table, as it would appear after running the conversion on PDF documents (i.e., with the same columns and structure). \n",
    "\n",
    "Once formatted, we upload this data to Snowflake so it is ready for the first step of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d79ed-ce37-4961-b4f9-62c1ef039b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0\n",
    "# -- Corpus conversion (e.g. PDF to MarkDown).\n",
    "\n",
    "\n",
    "if data_source == 'pdf':\n",
    "    # Form the SQL call for the execute_convert_corpus using the default parameters.\n",
    "    extraction_call = f\"\"\"CALL RAI_GRS.app.execute_convert_corpus('{project_id}');\"\"\"\n",
    "    \n",
    "    # Or if the documents contain visual content, we could run this process (note that it may be costly due to extensive LLM calls): \n",
    "    # extraction_call = f\"\"\"CALL RAI_GRS.app.execute_llm_convert_corpus('{project_id}');\"\"\"\n",
    "    \n",
    "    # Execute the SQL call.\n",
    "    cursor.execute(extraction_call)\n",
    "    \n",
    "    # See the results of the call.\n",
    "    results = cursor.fetchall() \n",
    "    for row in results:\n",
    "        print(row)\n",
    "else:\n",
    "    # Corpus convertion no needed here as we already have the raw text in a CSV file, so we will provide the corpus table.\n",
    "    # But if we had uploaded some PDF documents in a folder, then we sould run the following statement:\n",
    "    # We read the table of interest that has the textual data. \n",
    "    db_name = \"TF_DB\" \n",
    "    schema_name = \"TF_SCHEMA\" \n",
    "    data_table_name = \"VIRTUALS_AGENTS\"\n",
    "    sql = f\"SELECT * FROM {db_name}.{schema_name}.{data_table_name};\"\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n",
    "    virtual_agents_raw = cursor.fetch_pandas_all()\n",
    "    # In this experiment, we extract the name, symbol, and description of the agents, and combine them into a single string.\n",
    "    # This aggregated text will serve as the input document (one for each agent) for our algorithm.\n",
    "    corpus = virtual_agents_raw[[\"NAME\", \"SYMBOL\", \"DESCRIPTION\"]].copy()\n",
    "    \n",
    "    # Create the necessary columns to match the schema of the Snowflake corpus table.\n",
    "    corpus[\"PROJECT_ID\"] = project_id\n",
    "    corpus[\"CHUNK_ID\"] = corpus[\"NAME\"].apply(lambda x: f\"{project_id}/{x}\")\n",
    "    \n",
    "    # We could have custom metadata for the different documents, or let the LLM generate some (e.g. short title) but for now we use the same \n",
    "    # metadata for all the records. Note that if the \n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    formatted_time = now_utc.strftime(\"D:%Y%m%d%H%M%SZ\")\n",
    "    metadata_for_all_entries = {\n",
    "      \"creationDate\": formatted_time,\n",
    "      \"subject\": \"Text with descriptions of the Tokenflow agents.\",\n",
    "      \"source\": data_table_name\n",
    "    }\n",
    "    corpus[\"METADATA\"] = corpus.apply(lambda _: metadata_for_all_entries, axis=1)\n",
    "    corpus[\"CONTENT\"] = corpus.apply(lambda row: f\"Agent with name {row['NAME']} has symbol: {row['SYMBOL']} and description: {row['DESCRIPTION']}.\", axis=1)\n",
    "    \n",
    "    # The final table format is matches the CORPUS table schema on Snowflake.\n",
    "    final_corpus_df = corpus[[\"PROJECT_ID\", \"CHUNK_ID\", \"CONTENT\", \"METADATA\"]]\n",
    "    # Now that we have ready the textual data into the corpus table, we upload it to SF instead of running the corpus conversion step.\n",
    "    # Then we can run the GraphRAG Native App pipeline.\n",
    "    \n",
    "    # Upload corpus table to snowflake usint write_pandas from Snowflake API.\n",
    "    success, nchunks, nrows, _ = write_pandas(conn=conn,\n",
    "                                              df=final_corpus_df,\n",
    "                                              database=f'RAI_GRS',\n",
    "                                              schema='DATA',\n",
    "                                              table_name='CORPUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36de125-0001-4995-b748-cc14911e7b0a",
   "metadata": {},
   "source": [
    "#### 1. Entities and relations extraction \n",
    "\n",
    "After preparing the text documents, we can begin our pipeline with the first actual step of the knowledge graph (KG) construction. \n",
    "We are using the default prompt that comes with the installation of the app‚Äîit has been written to perform well across a variety of document types and domains. This process may take some time, depending on the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df9004-7bb5-48c7-9e84-d3895c92b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# -- Entities / relations extraction.\n",
    "\n",
    "# Form the SQL call.\n",
    "extraction_call = f\"\"\"CALL RAI_GRS.app.execute_get_entities_relations('{project_id}',\n",
    "                                                                                  '{llm_family}',\n",
    "                                                                                  '{completion_model}',\n",
    "                                                                                  '{is_fine_tuned_completion_model}',\n",
    "                                                                                  '{openai_api_key}');\"\"\"\n",
    "                                                                             \n",
    "\n",
    "# Execute the SQL call.\n",
    "cursor.execute(extraction_call)\n",
    "\n",
    "# See the results of the call.\n",
    "results = cursor.fetchall() \n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd38ad3-9a51-4233-bead-5cc0ab4dc493",
   "metadata": {},
   "source": [
    "#### 2. Community detection\n",
    "\n",
    "After extracting entities and relations, the next step is community detection. Several algorithms are available for this task, with configurable parameters. For example, you can set a maximum community size to prevent the formation of overly large communities with too many nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7848a-27bf-4e20-b149-dcd434acf737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# -- Community detection.\n",
    "\n",
    "# Form the SQL call with some parameters.\n",
    "params = {\n",
    "    \"algorithm\": \"leiden\",\n",
    "    \"initial_membership\": None,\n",
    "    \"weights\": None,\n",
    "    \"n_iterations\": 2,\n",
    "    \"max_comm_size\": 0,\n",
    "    \"seed\": None\n",
    "}\n",
    "append_flag = False\n",
    "# Convert parameters dict to JSON string if needed\n",
    "params_json = json.dumps(params)\n",
    "extraction_call = f\"\"\"CALL RAI_GRS.app.execute_get_communities(\n",
    "                                                                       '{project_id}',\n",
    "                                                                        PARSE_JSON('{params_json}'),\n",
    "                                                                        {str(append_flag).upper()}\n",
    "                                                                    );\"\"\"\n",
    "\n",
    "# Execute the SQL call.\n",
    "cursor.execute(extraction_call)\n",
    "\n",
    "# See the results of the call.\n",
    "results = cursor.fetchall() \n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2097132-8fb4-453e-9de3-cf9d103e3b61",
   "metadata": {},
   "source": [
    "#### 3. Graph indexing: summarization and embeddings\n",
    "\n",
    "This step performs the following operations:\n",
    "\n",
    "- Summarization of each community with LLM, capturing the context of the nodes it contains  \n",
    "- Embedding generation for the `CORPUS` table, node and edge properties, and the community summaries\n",
    "\n",
    "As with any LLM task of your pipeline, you can adjust the summarization prompt to guide the LLM on the desired level of abstraction and which details to include in the summaries. In this case, we use the default prompt. Note that this step is time-consuming, as it involves two separate processes with multiple AI model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159817f-9564-4d32-94ce-5c410dbfbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# -- Graph indexing: summarization and embeddings.\n",
    "\n",
    "# Form the SQL call.\n",
    "extraction_call = f\"\"\"CALL RAI_GRS.app.execute_get_embeddings('{project_id}',\n",
    "                                                                          '{llm_family}',\n",
    "                                                                          '{completion_model}',\n",
    "                                                                          '{embeddings_model}',\n",
    "                                                                          '{summarization_context}',\n",
    "                                                                          '{is_fine_tuned_completion_model}',\n",
    "                                                                          '{openai_api_key}');\"\"\"\n",
    "\n",
    "\n",
    "# Execute the SQL call.\n",
    "cursor.execute(extraction_call)\n",
    "\n",
    "# See the results of the call.\n",
    "results = cursor.fetchall() \n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23340232-d6a8-408f-ac33-899932443209",
   "metadata": {},
   "source": [
    "#### 4. Question Answering (QA)\n",
    "\n",
    "After indexing completes, we are ready to use our app for question answering!\n",
    "\n",
    "Note that the indexing phase occurs not only during embedding generation with your selected embedder but also within the Cortex Search service.\n",
    "\n",
    "This provides two retrieval options:\n",
    "- **Vector search**, which uses the embeddings generated in the previous step (step #3)  \n",
    "- **Cortex Search**, the managed service provided by Snowflake Cortex, which performs hybrid retrieval and reranking behind the scenes\n",
    "\n",
    "You can also select which sources to include in the retrieved results, since there are multiple content types to search: corpus items, community summaries, and verbalized properties of both nodes and edges. For now, we use the default behavior, which searches the most relevant across all available sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984be40-0084-4f04-95e1-9fb41df020bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "# Form the SQL call.\n",
    "# question = \"What is the meaning of the context?\"\n",
    "question = \"What do you know about WAI Combinator?\"\n",
    "\n",
    "extraction_call = f\"\"\"CALL RAI_GRS.app.execute_get_answer('{project_id}', '{question}');\"\"\"\n",
    "\n",
    "# Execute the SQL call.\n",
    "cursor.execute(extraction_call)\n",
    "\n",
    "# See the results of the call.\n",
    "result = cursor.fetchone() \n",
    "answer = result[1]\n",
    "context = json.loads(result[2])\n",
    "print(f\"Question: {question}\")\n",
    "print()\n",
    "print(f\"Answer: {answer}\")\n",
    "print()\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"Context has {len(context)} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03626b8-ffd5-493c-9c39-38637129004a",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "#### Load extracted graph data from snowflake \n",
    "\n",
    "The pipeline run produces graph data such as nodes, edges, and their properties, and stores them in Snowflake tables. After running the pipeline, we can download this graph data for use in other applications or purposes. In this case, we get back the nodes and edges to create a simple visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c55c9-9e44-40c4-8e84-2b61170c5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the provided method from Snowflake Python connector to get the graph data:\n",
    "# https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#fetch_pandas_all\n",
    "\n",
    "cursor.execute(f\"SELECT * FROM RAI_GRS.DATA.NODES WHERE PROJECT_ID='{project_id}'\")\n",
    "# Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n",
    "nodes = cursor.fetch_pandas_all()\n",
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a42ab-9830-48de-a4d7-739bdaec6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT * FROM RAI_GRS.DATA.EDGES WHERE PROJECT_ID='{project_id}'\")\n",
    "edges = cursor.fetch_pandas_all()\n",
    "edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5e41e-3362-423c-bc47-1a0abf76b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00393b-98bd-497f-bc79-7525cffbd9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_style(node_type):\n",
    "    \"\"\"Get appropriate icon and color based on node type with grouped synonyms.\"\"\"\n",
    "    \n",
    "    grouped_styles = {\n",
    "        # You need to make sure every key in grouped_styles is a tuple, even if it only has one item. In Python, \n",
    "        # that means you must include a trailing comma e.g. ('employer',): ('üë§üíº', '#FFB6C1'),:\n",
    "        ('ai_agent', 'agent', 'ai', 'ai_technology', 'ai_framework',): ('ü§ñ', '#FFB6C1'),\n",
    "        ('person', 'user', 'family_member',): ('üßë', '#DAA06D'), \n",
    "        ('platform', 'software', 'technology', 'feature',): ('üñ•Ô∏è', '#98FB98'),\n",
    "        ('blockchain', 'cryptocurrency', 'trading_platform', 'token', 'blockchain_paradise', 'meme_coin',): ('‚Çø', '#4682B4'),\n",
    "        ('ai_agent_role',): ('ü¶æ', '#FFD700'),\n",
    "        ('financial_product', 'currency',): ('üí∞', '#FFA500'),\n",
    "        ('company', 'organization',): ('üè¢', '#90EE90'),\n",
    "        ('document',): ('üìÑ', '#ADD8E6'),\n",
    "        ('legal_code', 'law', 'legislation', 'legal_document', 'legal_act'): ('‚öñÔ∏è', '#B0C4DE'), \n",
    "        ('country', 'place', 'ecosystem', 'location', 'territory',): ('üåç', '#DDA0DD'),\n",
    "        ('certification',): ('üéñÔ∏è', '#DC143C'),\n",
    "        ('regulation', 'legal', 'protocol', 'algorithm',): ('üìú', '#DDA0DD'),\n",
    "        ('default',): ('üìå', '#F0F0F0')\n",
    "    }\n",
    "\n",
    "    # Flatten into a usable dictionary\n",
    "    styles = {\n",
    "        synonym.lower(): style\n",
    "        for keys, style in grouped_styles.items()\n",
    "        for synonym in keys\n",
    "    }\n",
    "\n",
    "    return styles.get(node_type.lower(), styles['default'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e90e6-3f81-4a4d-ace5-b22761d9b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the nodes DataFrame to include only those nodes that are part of any edge (either as a source or destination in the edges DataFrame).\n",
    "\n",
    "# Step 1: Get all node IDs involved in edges (both source and destination)\n",
    "edge_node_ids = set(edges['SRC_NODE_ID']).union(set(edges['DST_NODE_ID']))\n",
    "\n",
    "# Step 2: Filter the nodes DataFrame to keep only rows where ID is in edge_node_ids\n",
    "filtered_nodes = nodes[nodes['ID'].isin(edge_node_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f8c56-898b-4add-b2d7-85ca28fce3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "for index, row in filtered_nodes.iterrows(): \n",
    "    node_id = row['ID']\n",
    "    node_type = row['TYPE']\n",
    "    entity_emoji, entity_color = get_node_style(node_type=row['TYPE']) \n",
    "    entity_label = f\"{entity_emoji} {row['ID']}\"\n",
    "    G.add_node(node_id, \n",
    "               label=entity_label,\n",
    "           type=node_type,\n",
    "           color=entity_color,\n",
    "           size=20)\n",
    "\n",
    "\n",
    "for index, row in edges.iterrows():        \n",
    "    G.add_edge(row['SRC_NODE_ID'], row['DST_NODE_ID'], label=row['TYPE'], size=1, length=500)\n",
    "\n",
    "graph = gv.d3(\n",
    "    G,\n",
    "    node_label_data_source='label', \n",
    "    show_edge_label=True, \n",
    "    edge_label_data_source='label',  \n",
    "    graph_height=500,   \n",
    "    links_force_distance=120,     \n",
    "    many_body_force_strength=-50,    \n",
    "    edge_label_size_factor=1.2,\n",
    ")\n",
    "\n",
    "# Display the interactive graph\n",
    "graph.display(inline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
